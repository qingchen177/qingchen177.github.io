---
title: "Attention Is All You Need | åœ£ç»å­¦ä¹ "
layout: post
tag: ["Attention", "Transformer" ,"Google"]
categories: ["äººå·¥æ™ºèƒ½"]
---

## å‰è¨€

è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/1706.03762

> è¿™ç¯‡æœ‰äº›åœ°æ–¹è¯»èµ·æ¥åº”è¯¥æ˜¯å¥‡å¥‡æ€ªæ€ªçš„ï¼Œæ˜¯å› ä¸ºæœ‰äº›åè¯è‡ªå·±è¯‘çš„ :(
>
> `dominant`ï¼šå ä¸»å¯¼åœ°ä½çš„
>
> `transduction`ï¼šè½¬å¯¼ï¼Œè½¬æ¢
>
> `firmly`ï¼šåšå®šåœ°ï¼Œåšå†³åœ°ï¼Œåšå›ºåœ°
>
> `identical`ï¼šå®Œå…¨ç›¸åŒçš„
>
> `facilitate`ï¼šä¿ƒè¿›ï¼›ä½¿ä¾¿åˆ©ï¼›ä¿ƒä½¿
>
> `consists`ï¼šåŒ…å«
>
> `compatibility`ï¼šå…¼å®¹æ€§ï¼›ç›¸å®¹æ€§ï¼›ç›¸å®¹ï¼›å¹¶å­˜ï¼›å’Œç¦ç›¸å¤„
>
> `counteract`ï¼šæŠµæ¶ˆï¼ŒæŠµæ¶ˆ

## åœ£å›¾

![image-20250722174506102](/assets/images/post/image-20250722174506102.png)

## ç¼–ç å™¨ï¼ˆEncoderï¼‰

$N=6$

ç”±6ä¸ªç›¸åŒçš„å±‚ï¼ˆ`layer`ï¼‰ç»„æˆï¼Œæ¯å±‚åŒ…å«ä¸¤ä¸ªå­å±‚ã€‚

å­å±‚ï¼š

1. å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼ˆ`multi-head self-attention mechanism`ï¼‰
2. ä½ç½®ç›¸å…³çš„å…¨è¿æ¥å‰é¦ˆç½‘ç»œå±‚ï¼ˆ`position-wise fully connected feed-forward network`ï¼‰

ä¸¤ä¸ªå­å±‚å‘¨å›´éƒ½ä½¿ç”¨äº†æ®‹å·®è¿æ¥ï¼ˆ`residual connection`ï¼‰éšåè¿›è¡Œäº†å±‚å½’ä¸€åŒ–ï¼ˆ`layer normalization`ï¼‰

æ¯ä¸ªå­å±‚çš„è¾“å‡ºæ˜¯$LayerNorm(x+Sublayer(x))$ï¼Œ$Sublayer(x)$æ˜¯å­å±‚æœ¬èº«å®ç°çš„å‡½æ•°ã€‚

ä¸ºäº†ä¾¿äºæ®‹å·®è¿æ¥ï¼Œæ¨¡å‹æ‰€æœ‰å­å±‚å’ŒåµŒå…¥å±‚ï¼ˆ`embedding layer`ï¼‰éƒ½äº§å‡º$d_{model}=512$ç»´åº¦çš„è¾“å‡ºã€‚

## è§£ç å™¨ï¼ˆDecoderï¼‰

è§£ç å™¨åŒæ ·ç”±6ä¸ªç›¸åŒçš„å±‚ç»„æˆï¼ŒåŒ…å«ä¸‰ä¸ªå­å±‚ã€‚

å­å±‚ï¼š

1. æ©ç å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼ˆ`masking multi-head self-attention mechanism`ï¼‰
2. å¯¹ç¼–ç å™¨çš„è¾“å‡ºæ‰§è¡Œå¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼ˆ`multi-head self-attention mechanism`ï¼‰
3. ä½ç½®ç›¸å…³çš„å…¨è¿æ¥å‰é¦ˆç½‘ç»œå±‚ï¼ˆ`position-wise fully connected feed-forward network`ï¼‰

## æ³¨æ„åŠ›ï¼ˆAttentionï¼‰

> An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.
>
> æ³¨æ„åŠ›å‡½æ•°å¯ä»¥è¢«æè¿°æˆå°†æŸ¥è¯¢å’Œä¸€ç»„é”®å€¼å¯¹æ˜ å°„åˆ°è¾“å‡ºï¼Œå…¶ä¸­æŸ¥è¯¢ã€é”®ã€å€¼å’Œè¾“å‡ºéƒ½æ˜¯å‘é‡ã€‚è¾“å‡ºæ˜¯å€¼çš„åŠ æƒå’Œï¼Œå…¶ä¸­åˆ†é…ç»™æ¯ä¸ªå€¼çš„æƒé‡æ˜¯é€šè¿‡æŸ¥è¯¢ä¸ç›¸åº”çš„é”®çš„å…¼å®¹æ€§å‡½æ•°è®¡ç®—çš„ã€‚

### ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›ï¼ˆScaled Dot-Product Attentionï¼‰

![image-20250723113710161](/assets/images/post/image-20250723113710161.png)

> The input consists of queries and keys of dimension $d_k$ , and values of dimension $d_v$ . We compute the dot products of the query with all keys, divide each by $\sqrt{d_k}$â€‹ , and apply a softmax function to obtain the weights on the values.
>
> è¾“å…¥ç”±ç»´åº¦ä¸º$d_k$çš„æŸ¥è¯¢å’Œé”®ä»¥åŠç»´åº¦ä¸º$d_v$çš„å€¼ç»„æˆã€‚è®¡ç®—æŸ¥è¯¢ä¸æ‰€æœ‰é”®çš„ç‚¹ç§¯,å°†æ¯ä¸ªç‚¹ç§¯é™¤ä»¥$\sqrt{d_k}$â€‹,å¹¶åº”ç”¨`softmax`å‡½æ•°ä»¥è·å¾—å€¼çš„æƒé‡ã€‚

$$
Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V
$$

åŒæ—¶è®¡ç®—ä¸€ç»„æŸ¥è¯¢çš„æ³¨æ„åŠ›å‡½æ•°ï¼ŒæŸ¥è¯¢æ‰“åŒ…æˆä¸€ä¸ªçŸ©é˜µQï¼Œé”®å’Œå€¼ä¹Ÿæ‰“åŒ…æˆçŸ©é˜µKå’ŒVï¼Œç»å…¸å…¬å¼å¦‚ä¸Šã€‚

> æœ€å¸¸ç”¨çš„ä¸¤ç§æ³¨æ„åŠ›å‡½æ•°æ˜¯ï¼š
>
> 1. `additive attention`ï¼ˆåŠ æ€§æ³¨æ„åŠ›ï¼‰
> 2. `dot-productï¼ˆmultiplicativeï¼‰attention`ï¼ˆç‚¹ç§¯æ³¨æ„åŠ›ã€ä¹˜æ³•æ³¨æ„åŠ›ï¼‰
>
> äºŒè€…ä¹‹é—´ï¼š
>
> - ç‚¹ç§¯æ³¨æ„åŠ›å’Œä»–ä»¬çš„ç®—æ³•ä¸€æ ·ï¼Œé™¤äº†ç¼©æ”¾å› å­$\frac{1}{\sqrt{d_k}}$â€‹
> - åŠ æ€§æ³¨æ„åŠ›ä½¿ç”¨æœ‰å•ä¸ªéšè—å±‚çš„å‰é¦ˆç½‘ç»œæ¥è®¡ç®—å…¼å®¹æ€§å‡½æ•°
>
> ç†è®ºå¤æ‚åº¦ä¸Šç›¸ä¼¼ï¼Œä½†æ˜¯ç‚¹ç§¯å®è·µä¸­æ›´å¿«æ›´çœç©ºé—´ï¼Œå› ä¸ºçŸ©é˜µä¹˜æ³•çš„ä¼˜åŒ–ä»£ç 
>
> è¿™é‡Œç‰¹åˆ«è¯´äº†$d_k$çš„å€¼å¤§å°ï¼ŒåŸè¯ï¼š
>
> While for small values of $d_k$ the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of $d_k$. We suspect that for large values of $d_k$ , the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients. To counteract this effect, we scale the dot products by $\frac{1}{\sqrt{d_k}}$.
>
> $d_k$çš„å€¼è¿‡å¤§æ—¶ï¼ŒåŠ æ€§æ³¨æ„åŠ›ä¼˜äºæ²¡æœ‰ç¼©æ”¾çš„ç‚¹ç§¯æ³¨æ„åŠ›ï¼Œ`suspect?`ï¼ˆçŒœï¼‰æ€€ç–‘æ˜¯ç‚¹ç§¯çš„æ•°å€¼å˜å¤§ï¼Œä½¿softmaxå‡½æ•°çš„æ¢¯åº¦è¿›å…¥æå°çš„åŒºåŸŸï¼Œä¸ºäº†æŠµæ¶ˆå½±å“å°±æŠŠç‚¹ç§¯ç¼©æ”¾ã€‚
>
> ç¼©æ”¾å› å­$\frac{1}{\sqrt{d_k}}$ç›®çš„æ˜¯è®©ç‚¹ç§¯çš„**æ–¹å·®ä¿æŒç¨³å®š** ï¼Œä¸ä¼šéš $d_k$â€‹â€‹â€‹ å¢é•¿è€Œçˆ†ç‚¸ã€‚
>
> ç»å…¸æè¿°ï¼š
>
> - **Query (q)** ï¼šæˆ‘åœ¨æ‰¾ä»€ä¹ˆï¼Ÿ
> - **Key (k)** ï¼šæˆ‘æ˜¯ä»€ä¹ˆï¼Ÿ
> - **Value (v)** ï¼šæˆ‘çš„ä¿¡æ¯æ˜¯ä»€ä¹ˆï¼Ÿ



å¦‚æœä¸ç¼©æ”¾å¯èƒ½ä¼šå‡ºç°`[[0.99, 0.01], [0.01, 0.99]]`è¿™æ ·çš„æ³¨æ„åŠ›å¾—åˆ†çŸ©é˜µï¼Œç¬¬ä¸€ä¸ª`token`åªå…³æ³¨è‡ªå·±ï¼Œç¬¬äºŒä¸ªä¹Ÿæ˜¯ï¼Œå’Œæ—è¾¹æ²¡æœ‰è”ç³»ï¼Œç­‰äºè¯´æ²¡æœ‰æ³¨æ„åŠ›äº†ã€‚

åŒæ—¶æç«¯å¾—åˆ†ä¼šè®© `softmax` æ¢¯åº¦æ¥è¿‘ 0ï¼Œæ¨¡å‹éš¾ä»¥è®­ç»ƒï¼ˆæ¢¯åº¦æ¶ˆå¤±ï¼‰

ç¼©æ”¾é€šè¿‡å½’ä¸€åŒ–æ–¹å·®ï¼Œè®©æ³¨æ„åŠ›å¾—åˆ†ä¿æŒåœ¨åˆç†åŒºé—´ï¼Œç¡®ä¿ `softmax` å’Œåå‘ä¼ æ’­æ­£å¸¸å·¥ä½œã€‚

**ä¸ªäººç†è§£**ï¼šç»™ä¸€å¥è¯`å“ˆåŸºå’ªå—åŒ—ç»¿è±†`ï¼Œç»è¿‡`tokenizer`å˜æˆåºåˆ—`[133, 23, 58]`

| char   | token |
| ------ | ----- |
| å“ˆåŸºå’ª | 133   |
| å—åŒ—   | 23    |
| ç»¿è±†   | 58    |

`token`é€šè¿‡åµŒå…¥çŸ©é˜µå˜æˆå‘é‡

| token | embeddings                    |
| ----- | ----------------------------- |
| 133   | [ 1.033 -0.077  1.943 -1.262] |
| 23    | [ 1.22 -0.077  1.943 -1.262]  |
| 58    | [ 0.033 -0.077  1.943 -1.262] |

ç„¶åè®¾ç½®ä¸‰ä¸ªæƒé‡çŸ©é˜µ$W_Q,W_K,W_V$ä¸å‘é‡ç›¸ä¹˜å¾—åˆ°$Q,K,V$â€‹

ç„¶å$QÂ·K^T$å¾—åˆ°æ³¨æ„åŠ›çš„æƒé‡çŸ©é˜µ

ç„¶å$softmax(æƒé‡çŸ©é˜µ)$ç›¸å½“äºå½’ä¸€åŒ–ï¼Ÿåæ­£å˜æˆæ¯è¡ŒåŠ èµ·æ¥å’Œä¸º1äº†

æœ€åä¹˜ä¸ª$V$â€‹å°±å¾—åˆ°ç»è¿‡æ³¨æ„åŠ›çš„å‘é‡è¡¨ç¤ºäº†ã€‚

å†™ä¸ªç®€å•çš„ä»£ç ï¼š

```python
# å“ˆåŸºå’ª - 133
# å—åŒ— - 23
# ç»¿è±† - 58
tokens = [133, 23, 58]
# é€šè¿‡åµŒå…¥çŸ©é˜µæ˜ å°„ä¸ºå‘é‡
# æ¨¡å‹ç»´åº¦
model_dim = 4
# å‡è®¾è¯å…¸å¤§å°ä¸º1000ï¼Œåˆ›å»ºåµŒå…¥çŸ©é˜µï¼ˆå®é™…ä¸­é€šè¿‡è®­ç»ƒå­¦ä¹ ï¼‰
vocab_size = 1000
embedding_matrix = np.random.randn(vocab_size, model_dim) # 100,8çš„æ˜ å°„
# è¯´ç™½äº†HashMapæ‰¾idå¯¹åº”çš„å‘é‡
# å°†token IDæ˜ å°„ä¸ºå‘é‡ï¼ˆè¿™é‡Œç®€åŒ–ä¸ºç›´æ¥ç´¢å¼•ï¼‰
token_embeddings = np.array([embedding_matrix[idx] for idx in tokens])

print(f"åµŒå…¥çŸ©é˜µå½¢çŠ¶: {embedding_matrix.shape}")
print(f"token_embeddingså½¢çŠ¶: {token_embeddings.shape}")  # [3, 8]
print(f"å“ˆåŸºå’ªçš„åµŒå…¥å‘é‡: {token_embeddings[0].round(3)}")

# åˆå§‹åŒ– Q K V
# d_k = 2
# åœ¨å®é™…åº”ç”¨ä¸­ï¼ŒQã€Kã€Væ˜¯é€šè¿‡åµŒå…¥çŸ©é˜µä¸æƒé‡çŸ©é˜µç›¸ä¹˜å¾—åˆ°çš„
# å®šä¹‰æƒé‡çŸ©é˜µï¼ˆå®é™…ä¸­é€šè¿‡è®­ç»ƒå­¦ä¹ å¾—åˆ°ï¼‰
W_q = np.array([
    [0.2, 0.1],
    [0.3, 0.5],
    [0.4, 0.6],
    [0.1, 0.2]
])  # (d_model, d_k)

W_k = np.array([
    [0.1, 0.3],
    [0.2, 0.4],
    [0.5, 0.7],
    [0.1, 0.2]
])  # (d_model, d_k)

W_v = np.array([
    [1.0, 0.0, 0.0, 0.0],
    [0.0, 1.0, 0.0, 0.0],
    [0.0, 0.0, 1.0, 0.0],
    [0.0, 0.0, 0.0, 1.0]
])  # (d_model, d_v)ï¼Œè¿™é‡Œç®€åŒ–ä¸ºå•ä½çŸ©é˜µ

# é€šè¿‡åµŒå…¥çŸ©é˜µä¸æƒé‡çŸ©é˜µç›¸ä¹˜ç”ŸæˆQã€Kã€V
Q = np.dot(token_embeddings, W_q)  # (seq_len, d_k)
K = np.dot(token_embeddings, W_k)  # (seq_len, d_k)
V = np.dot(token_embeddings, W_v)  # (seq_len, d_v)

print("ä»embeddingsç”Ÿæˆçš„QçŸ©é˜µï¼š")
print(Q)
print("\nä»embeddingsç”Ÿæˆçš„KçŸ©é˜µï¼š")
print(K)
print("\nä»embeddingsç”Ÿæˆçš„VçŸ©é˜µï¼š")
print(V)


def scaled_dot_product_attention(Q, K, V, d_k):
    # è®¡ç®—æ³¨æ„åŠ›å¾—åˆ†ï¼Œä¹Ÿå°±æ˜¯ç›¸ä¼¼åº¦
    scores = np.dot(Q, K.T)  # (seq_len, seq_len)ï¼Œæ¯ä¸ªå…ƒç´ è¡¨ç¤ºä¸¤ä¸ªtokençš„å…³è”åº¦
    print("æ³¨æ„åŠ›å¾—åˆ†çŸ©é˜µï¼ˆQ*K^Tï¼‰ï¼š")
    print(np.round(scores, 3))

    # 2. ç¼©æ”¾ï¼ˆé˜²æ­¢å€¼è¿‡å¤§å¯¼è‡´softmaxæ¢¯åº¦æ¶ˆå¤±ï¼‰
    scaled_scores = scores / np.sqrt(d_k)
    print("\nç¼©æ”¾åçš„æ³¨æ„åŠ›å¾—åˆ†ï¼ˆé™¤ä»¥âˆšd_kï¼‰ï¼š")
    print(np.round(scaled_scores, 3))

    # 3. softmaxè®¡ç®—æ³¨æ„åŠ›æƒé‡ï¼ˆæ¯è¡Œå’Œä¸º1ï¼Œè¡¨ç¤ºå¯¹å…¶ä»–tokençš„å…³æ³¨æ¯”ä¾‹ï¼‰
    attention_weights = np.exp(scaled_scores) / np.sum(np.exp(scaled_scores), axis=1, keepdims=True)
    print("\næ³¨æ„åŠ›æƒé‡çŸ©é˜µï¼š")
    print(np.round(attention_weights, 3))

    # 4. åŠ æƒæ±‚å’Œï¼ˆç”¨æƒé‡å¯¹Vè¿›è¡ŒåŠ æƒï¼Œå¾—åˆ°æœ€ç»ˆè¾“å‡ºï¼‰
    output = np.dot(attention_weights, V)
    return output, attention_weights


# è®¡ç®—æ³¨æ„åŠ›ï¼ˆd_k=2ï¼‰
output, attn_weights = scaled_dot_product_attention(Q, K, V, d_k=2)

# ç»“æœè§£è¯»
print("\n===== ç»“æœè§£è¯» =====")
print("è¾“å…¥tokenï¼š", ["å“ˆåŸºå’ª", "å—åŒ—", "ç»¿è±†"])
print("æ¯ä¸ªtokençš„æ³¨æ„åŠ›è¾“å‡ºï¼ˆèåˆäº†å…¶ä»–ç›¸å…³tokençš„ä¿¡æ¯ï¼‰ï¼š")
for i, token in enumerate(["å“ˆåŸºå’ª", "å—åŒ—", "ç»¿è±†"]):
    print(f"{token}çš„è¾“å‡ºï¼š", np.round(output[i], 3))

print("\næ³¨æ„åŠ›æƒé‡å«ä¹‰ï¼š")
for i, token in enumerate(["å“ˆåŸºå’ª", "å—åŒ—", "ç»¿è±†"]):
    print(f"å½“å¤„ç†{token}æ—¶ï¼Œå¯¹å…¶ä»–tokençš„å…³æ³¨æ¯”ä¾‹ï¼š")
    for j, other_token in enumerate(["å“ˆåŸºå’ª", "å—åŒ—", "ç»¿è±†"]):
        print(f"  å¯¹{other_token}çš„å…³æ³¨ï¼š{attn_weights[i][j]:.3f}")
```

è¾“å‡ºï¼š

```python
åµŒå…¥çŸ©é˜µå½¢çŠ¶: (100, 4)
token_embeddingså½¢çŠ¶: (3, 4)
å“ˆåŸºå’ªçš„åµŒå…¥å‘é‡: [ 1.033 -0.077  1.943 -1.262]
ä»embeddingsç”Ÿæˆçš„QçŸ©é˜µï¼š
[[ 0.83469225  0.97844849]
 [-0.22140911 -0.50136356]
 [-0.38048561 -0.49219428]]

ä»embeddingsç”Ÿæˆçš„KçŸ©é˜µï¼š
[[ 0.93336044  1.38711376]
 [-0.4832162  -0.40850584]
 [-0.40378534 -0.55737316]]

ä»embeddingsç”Ÿæˆçš„VçŸ©é˜µï¼š
[[ 1.0333236  -0.07687391  1.94313157 -1.26162928]
 [ 1.18221604 -0.0298283  -1.46568319  1.37369452]
 [-0.13959719 -0.50792964 -0.88052409  1.52022357]]
æ³¨æ„åŠ›å¾—åˆ†çŸ©é˜µï¼ˆQ*K^Tï¼‰ï¼š
[[ 2.136 -0.803 -0.882]
 [-0.902  0.312  0.369]
 [-1.038  0.385  0.428]]

ç¼©æ”¾åçš„æ³¨æ„åŠ›å¾—åˆ†ï¼ˆé™¤ä»¥âˆšd_kï¼‰ï¼š
[[ 1.511 -0.568 -0.624]
 [-0.638  0.22   0.261]
 [-0.734  0.272  0.303]]

æ³¨æ„åŠ›æƒé‡çŸ©é˜µï¼š
[[0.804 0.101 0.095]
 [0.172 0.406 0.422]
 [0.153 0.417 0.43 ]]

===== ç»“æœè§£è¯» =====
è¾“å…¥tokenï¼š ['å“ˆåŸºå’ª', 'å—åŒ—', 'ç»¿è±†']
æ¯ä¸ªtokençš„æ³¨æ„åŠ›è¾“å‡ºï¼ˆèåˆäº†å…¶ä»–ç›¸å…³tokençš„ä¿¡æ¯ï¼‰ï¼š
å“ˆåŸºå’ªçš„è¾“å‡ºï¼š [ 0.937 -0.113  1.331 -0.732]
å—åŒ—çš„è¾“å‡ºï¼š [ 0.598 -0.24  -0.632  0.982]
ç»¿è±†çš„è¾“å‡ºï¼š [ 0.591 -0.243 -0.694  1.035]

æ³¨æ„åŠ›æƒé‡å«ä¹‰ï¼š
å½“å¤„ç†å“ˆåŸºå’ªæ—¶ï¼Œå¯¹å…¶ä»–tokençš„å…³æ³¨æ¯”ä¾‹ï¼š
  å¯¹å“ˆåŸºå’ªçš„å…³æ³¨ï¼š0.804
  å¯¹å—åŒ—çš„å…³æ³¨ï¼š0.101
  å¯¹ç»¿è±†çš„å…³æ³¨ï¼š0.095
å½“å¤„ç†å—åŒ—æ—¶ï¼Œå¯¹å…¶ä»–tokençš„å…³æ³¨æ¯”ä¾‹ï¼š
  å¯¹å“ˆåŸºå’ªçš„å…³æ³¨ï¼š0.172
  å¯¹å—åŒ—çš„å…³æ³¨ï¼š0.406
  å¯¹ç»¿è±†çš„å…³æ³¨ï¼š0.422
å½“å¤„ç†ç»¿è±†æ—¶ï¼Œå¯¹å…¶ä»–tokençš„å…³æ³¨æ¯”ä¾‹ï¼š
  å¯¹å“ˆåŸºå’ªçš„å…³æ³¨ï¼š0.153
  å¯¹å—åŒ—çš„å…³æ³¨ï¼š0.417
  å¯¹ç»¿è±†çš„å…³æ³¨ï¼š0.430
```

### å¤šå¤´æ³¨æ„åŠ›ï¼ˆMulti-Head Attentionï¼‰

![image-20250723113757893](/assets/images/post/image-20250723113757893.png)

ç›´æ¥ä¸€ä¸ªé•¿éš¾å¥èµ·æ‰‹ï¼š

> Instead of performing a single attention function with $d_{model}$ -dimensional keys, values and queries,we found it beneficial to linearly project the queries, keys and values $h$ times with different, learned linear projections to $d_k$ ,$d_k$ and $d_v$â€‹ dimensions, respectively.
>
> `respectively`ï¼šåˆ†åˆ«ï¼›å„è‡ª

æ€»ç»“å°±æ˜¯ï¼š`å“ˆåŸºå’ªå—åŒ—ç»¿è±†` è¿™å¥è¯é‚£æˆ‘é—®ä½ ï¼Ÿä½ æœ‰å‡ ç§ç†è§£ï¼Ÿé‚£æˆ‘é—®ä½ ï¼Ÿä½ çŸ¥é“æˆ‘è¯´è¿™å¥è¯ä»€ä¹ˆæ„æ€ï¼Ÿé‚£æˆ‘é—®ä½ ï¼ŸåŸºå“ˆåŸºå’ªæ¼«æ³¢ä½ åˆæ€ä¹ˆè§£é‡Šï¼Ÿè¿™æ³¢ä½ åœ¨ç¬¬å‡ å±‚ï¼Ÿ

å•å•ä¸€ä¸ªå±‚ç†è§£æ˜¯ä¸å¤Ÿçš„ï¼Œå¾—å¤šåŠ å‡ å±‚ç†è§£æ¥åˆ°å¤§æ°”å±‚ã€‚
$$
MultiHead(Q,K,V)=Concat(head_1,...,head_h)W^O
\\
where \ head_i = Attention(QW_i^Q,KW_i^k,VW^V_i)
$$
ç¤ºæ„å›¾ï¼š

![img](/assets/images/post/v2-968291dfb99ad3e989ce5ba3db1c6e99_1440w.jpg)æŠŠä¸Šé¢çš„ä»£ç æ”¹ä¸€ä¸‹å°±èƒ½çŸ¥é“å¤šå¤´æ³¨æ„åŠ›çš„åŸç†ï¼š

```python
# å“ˆåŸºå’ª - 133
# å—åŒ— - 23
# ç»¿è±† - 58
tokens = [133, 23, 58]
# é€šè¿‡åµŒå…¥çŸ©é˜µæ˜ å°„ä¸ºå‘é‡
# æ¨¡å‹ç»´åº¦
model_dim = 4

# å‡è®¾è¯å…¸å¤§å°ä¸º1000ï¼Œåˆ›å»ºåµŒå…¥çŸ©é˜µï¼ˆå®é™…ä¸­é€šè¿‡è®­ç»ƒå­¦ä¹ ï¼‰
vocab_size = 1000
embedding_matrix = np.random.randn(vocab_size, model_dim)

# å°†token IDæ˜ å°„ä¸ºå‘é‡ï¼ˆè¿™é‡Œç®€åŒ–ä¸ºç›´æ¥ç´¢å¼•ï¼‰
token_embeddings = np.array([embedding_matrix[idx] for idx in tokens])

print(f"åµŒå…¥çŸ©é˜µå½¢çŠ¶: {embedding_matrix.shape}")
print(f"token_embeddingså½¢çŠ¶: {token_embeddings.shape}")  # [3, 4]
print(f"å“ˆåŸºå’ªçš„åµŒå…¥å‘é‡: {token_embeddings[0].round(3)}")
# å¢åŠ batchç»´åº¦ä»¥é€‚åº”å‡½æ•°æ¥å£
token_embeddings_batch = token_embeddings[np.newaxis, :, :]  # (1, 3, 4)

# è®¾ç½®å¤šå¤´æ³¨æ„åŠ›å¤´æ•°æ˜¯2
head_num = 2

# åˆå§‹åŒ–å¤šå¤´æ³¨æ„åŠ›çš„ Q K V æƒé‡çŸ©é˜µ
# åœ¨å®é™…åº”ç”¨ä¸­ï¼ŒQã€Kã€Væ˜¯é€šè¿‡åµŒå…¥çŸ©é˜µä¸æƒé‡çŸ©é˜µç›¸ä¹˜å¾—åˆ°çš„
# å®šä¹‰æƒé‡çŸ©é˜µï¼ˆå®é™…ä¸­é€šè¿‡è®­ç»ƒå­¦ä¹ å¾—åˆ°ï¼‰
W_q = np.random.randn(model_dim, 8)  # [4, 8]
W_k = np.random.randn(model_dim, 8)  # [4, 8]
W_v = np.random.randn(model_dim, 16)  # [4, 16]
W_o = np.random.randn(16, model_dim)  # [16, 4]

# çº¿æ€§æŠ•å½±
Q = np.matmul(token_embeddings_batch, W_q)  # [1, 3, 8]
K = np.matmul(token_embeddings_batch, W_k)  # [1, 3, 8]
V = np.matmul(token_embeddings_batch, W_v)  # [1, 3, 16]
print("QçŸ©é˜µï¼š")
print(Q.shape)
print("KçŸ©é˜µï¼š")
print(K.shape)
print("VçŸ©é˜µï¼š")
print(V.shape)


# è¾…åŠ©å‡½æ•°ï¼šå°†çŸ©é˜µåˆ†å‰²ä¸ºå¤šä¸ªå¤´
def split_heads(x, head_num):
    """å°†è¾“å…¥åˆ†å‰²ä¸ºå¤šä¸ªå¤´"""
    batch_size, seq_len, d_model = x.shape
    d_k = d_model // head_num
    return x.reshape(batch_size, seq_len, head_num, d_k).transpose(0, 2, 1, 3)


# è¾…åŠ©å‡½æ•°ï¼šå°†å¤šå¤´ç»“æœåˆå¹¶
def combine_heads(x):
    """å°†å¤šä¸ªå¤´çš„ç»“æœåˆå¹¶"""
    batch_size, head_num, seq_len, d_k = x.shape
    d_model = head_num * d_k
    return x.transpose(0, 2, 1, 3).reshape(batch_size, seq_len, d_model)


def scaled_dot_product_attention(Q, K, V, mask=None):
    """è®¡ç®—ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›"""
    # è®¡ç®—æ³¨æ„åŠ›å¾—åˆ†ï¼Œä¹Ÿå°±æ˜¯ç›¸ä¼¼åº¦
    scores = np.matmul(Q, K.transpose(0, 1, 3, 2))  # (batch, heads, seq_len, seq_len)

    # ç¼©æ”¾ï¼ˆé˜²æ­¢å€¼è¿‡å¤§å¯¼è‡´softmaxæ¢¯åº¦æ¶ˆå¤±ï¼‰
    scaled_scores = scores / np.sqrt(Q.shape[-1])

    # åº”ç”¨æ©ç ï¼ˆå¦‚æœæä¾›ï¼‰
    if mask is not None:
        scaled_scores += (mask * -1e9)

    # softmaxè®¡ç®—æ³¨æ„åŠ›æƒé‡ï¼ˆæ¯è¡Œå’Œä¸º1ï¼Œè¡¨ç¤ºå¯¹å…¶ä»–tokençš„å…³æ³¨æ¯”ä¾‹ï¼‰
    attention_weights = np.exp(scaled_scores) / np.sum(np.exp(scaled_scores), axis=-1, keepdims=True)

    # åŠ æƒæ±‚å’Œï¼ˆç”¨æƒé‡å¯¹Vè¿›è¡ŒåŠ æƒï¼Œå¾—åˆ°æœ€ç»ˆè¾“å‡ºï¼‰
    output = np.matmul(attention_weights, V)
    return output, attention_weights


def multi_head_attention(W_o, head_num, mask=None):
    """å®ç°å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶"""
    # åˆ†å‰²ä¸ºå¤šå¤´
    Q_split = split_heads(Q, head_num)  # (batch, heads, seq_len, d_k)
    K_split = split_heads(K, head_num)  # (batch, heads, seq_len, d_k)
    V_split = split_heads(V, head_num)  # (batch, heads, seq_len, d_k)

    # è®¡ç®—æ¯ä¸ªå¤´çš„æ³¨æ„åŠ›
    output, attention_weights = scaled_dot_product_attention(Q_split, K_split, V_split, mask)

    # åˆå¹¶å¤šå¤´ç»“æœ
    output_combined = combine_heads(output)  # (batch, seq_len, d_model)

    # æœ€ç»ˆçº¿æ€§æŠ•å½±
    output_final = np.matmul(output_combined, W_o)  # (batch, seq_len, d_model)

    return output_final, attention_weights


# è®¡ç®—å¤šå¤´æ³¨æ„åŠ›
output, attn_weights = multi_head_attention(W_o, head_num)

# ç§»é™¤batchç»´åº¦ä»¥æ–¹ä¾¿å±•ç¤º
output = output[0]  # (3, 4)
attn_weights = attn_weights[0]  # (2, 3, 3)ï¼Œä¸¤ä¸ªå¤´çš„æ³¨æ„åŠ›æƒé‡

# ç»“æœè§£è¯»
print("\n===== å¤šå¤´æ³¨æ„åŠ›ç»“æœè§£è¯» =====")
print("è¾“å…¥tokenï¼š", ["å“ˆåŸºå’ª", "å—åŒ—", "ç»¿è±†"])
print("æ¯ä¸ªtokençš„å¤šå¤´æ³¨æ„åŠ›è¾“å‡ºï¼ˆèåˆäº†å…¶ä»–ç›¸å…³tokençš„ä¿¡æ¯ï¼‰ï¼š")
for i, token in enumerate(["å“ˆåŸºå’ª", "å—åŒ—", "ç»¿è±†"]):
    print(f"{token}çš„è¾“å‡ºï¼š", np.round(output[i], 3))

print("\næ¯ä¸ªå¤´çš„æ³¨æ„åŠ›æƒé‡å«ä¹‰ï¼š")
for h in range(head_num):
    print(f"\nç¬¬ {h + 1} ä¸ªå¤´çš„æ³¨æ„åŠ›æƒé‡:")
    for i, token in enumerate(["å“ˆåŸºå’ª", "å—åŒ—", "ç»¿è±†"]):
        print(f"  å½“å¤„ç†{token}æ—¶ï¼Œå¯¹å…¶ä»–tokençš„å…³æ³¨æ¯”ä¾‹ï¼š")
        for j, other_token in enumerate(["å“ˆåŸºå’ª", "å—åŒ—", "ç»¿è±†"]):
            print(f"    å¯¹{other_token}çš„å…³æ³¨ï¼š{attn_weights[h][i][j]:.3f}")
```

ç»“æœè¾“å‡ºï¼š

```python
åµŒå…¥çŸ©é˜µå½¢çŠ¶: (1000, 4)
token_embeddingså½¢çŠ¶: (3, 4)
å“ˆåŸºå’ªçš„åµŒå…¥å‘é‡: [ 0.889  0.958 -1.73  -0.727]
QçŸ©é˜µï¼š
(1, 3, 8)
KçŸ©é˜µï¼š
(1, 3, 8)
VçŸ©é˜µï¼š
(1, 3, 16)

===== å¤šå¤´æ³¨æ„åŠ›ç»“æœè§£è¯» =====
è¾“å…¥tokenï¼š ['å“ˆåŸºå’ª', 'å—åŒ—', 'ç»¿è±†']
æ¯ä¸ªtokençš„å¤šå¤´æ³¨æ„åŠ›è¾“å‡ºï¼ˆèåˆäº†å…¶ä»–ç›¸å…³tokençš„ä¿¡æ¯ï¼‰ï¼š
å“ˆåŸºå’ªçš„è¾“å‡ºï¼š [ -6.932   3.827   2.633 -11.445]
å—åŒ—çš„è¾“å‡ºï¼š [ 2.573 -0.541 -1.144  1.213]
ç»¿è±†çš„è¾“å‡ºï¼š [-5.032  3.05   1.803 -8.009]

æ¯ä¸ªå¤´çš„æ³¨æ„åŠ›æƒé‡å«ä¹‰ï¼š

ç¬¬ 1 ä¸ªå¤´çš„æ³¨æ„åŠ›æƒé‡:
  å½“å¤„ç†å“ˆåŸºå’ªæ—¶ï¼Œå¯¹å…¶ä»–tokençš„å…³æ³¨æ¯”ä¾‹ï¼š
    å¯¹å“ˆåŸºå’ªçš„å…³æ³¨ï¼š0.834
    å¯¹å—åŒ—çš„å…³æ³¨ï¼š0.012
    å¯¹ç»¿è±†çš„å…³æ³¨ï¼š0.154
  å½“å¤„ç†å—åŒ—æ—¶ï¼Œå¯¹å…¶ä»–tokençš„å…³æ³¨æ¯”ä¾‹ï¼š
    å¯¹å“ˆåŸºå’ªçš„å…³æ³¨ï¼š0.102
    å¯¹å—åŒ—çš„å…³æ³¨ï¼š0.747
    å¯¹ç»¿è±†çš„å…³æ³¨ï¼š0.151
  å½“å¤„ç†ç»¿è±†æ—¶ï¼Œå¯¹å…¶ä»–tokençš„å…³æ³¨æ¯”ä¾‹ï¼š
    å¯¹å“ˆåŸºå’ªçš„å…³æ³¨ï¼š0.496
    å¯¹å—åŒ—çš„å…³æ³¨ï¼š0.238
    å¯¹ç»¿è±†çš„å…³æ³¨ï¼š0.266

ç¬¬ 2 ä¸ªå¤´çš„æ³¨æ„åŠ›æƒé‡:
  å½“å¤„ç†å“ˆåŸºå’ªæ—¶ï¼Œå¯¹å…¶ä»–tokençš„å…³æ³¨æ¯”ä¾‹ï¼š
    å¯¹å“ˆåŸºå’ªçš„å…³æ³¨ï¼š0.961
    å¯¹å—åŒ—çš„å…³æ³¨ï¼š0.005
    å¯¹ç»¿è±†çš„å…³æ³¨ï¼š0.034
  å½“å¤„ç†å—åŒ—æ—¶ï¼Œå¯¹å…¶ä»–tokençš„å…³æ³¨æ¯”ä¾‹ï¼š
    å¯¹å“ˆåŸºå’ªçš„å…³æ³¨ï¼š0.265
    å¯¹å—åŒ—çš„å…³æ³¨ï¼š0.349
    å¯¹ç»¿è±†çš„å…³æ³¨ï¼š0.385
  å½“å¤„ç†ç»¿è±†æ—¶ï¼Œå¯¹å…¶ä»–tokençš„å…³æ³¨æ¯”ä¾‹ï¼š
    å¯¹å“ˆåŸºå’ªçš„å…³æ³¨ï¼š0.842
    å¯¹å—åŒ—çš„å…³æ³¨ï¼š0.052
    å¯¹ç»¿è±†çš„å…³æ³¨ï¼š0.106
```

å•å¤´å’Œå¤šå¤´çš„çŸ©é˜µç»“æ„å˜åŒ–å¯¹æ¯”ï¼š

```mermaid
graph LR
subgraph å•å¤´
direction LR
subgraph å•å¤´TOKENå‘é‡è¡¨ç¤º
embeddings[embed:3 4]
end

subgraph å•å¤´QKVæƒé‡çŸ©é˜µ
	WQ[WQ:4 8]
	WK[WK:4 8]
	WV[WV:4 4]
end

subgraph å•å¤´QKVçŸ©é˜µ
	Q[Q:3 8]
	K[K:3 8]
	V[V:3 4]
end

subgraph å•å¤´è®¡ç®—
a[Q*K^T:3 3 = æ³¨æ„åŠ›æƒé‡çŸ©é˜µ]
b[æ³¨æ„åŠ›æƒé‡çŸ©é˜µ * Vï¼š3 4]
end
end

å•å¤´TOKENå‘é‡è¡¨ç¤º --> å•å¤´QKVæƒé‡çŸ©é˜µ --> å•å¤´QKVçŸ©é˜µ --> å•å¤´è®¡ç®—
a --> b


subgraph å¤šå¤´
direction LR
subgraph TOKENå‘é‡è¡¨ç¤º
embeddings2[embed:3 4]
end

subgraph QKVæƒé‡çŸ©é˜µ
	WQ2[WQ:4 8]
	WK2[WK:4 8]
	WV2[WV:4 16]
	WO[WO:16 4]
end

subgraph QKVçŸ©é˜µ
	Q2[Q:3 8]
	K2[K:3 8]
	V2[V:3 16]
end

subgraph åˆ†å¤´è¡ŒåŠ¨Hä¸º2
	Q3[Q:2 3 4]
	K3[K:2 3 4]
	V3[V:2 3 8]
end



subgraph è®¡ç®—
a2[Q*K^T:2 3 3 = æ³¨æ„åŠ›æƒé‡çŸ©é˜µ] 
b2[æ³¨æ„åŠ›æƒé‡çŸ©é˜µ * Vï¼š2 3 8]
c[è¿™é‡Œæ¯”å•å¤´å¤šäº†ä¸€ä¸ª3*3çš„æ³¨æ„åŠ›æƒé‡çŸ©é˜µ]
d[2 3 8 -> 3 16 * WO: 16 4 æœ€åå¾—åˆ° 3 4]
end
end

TOKENå‘é‡è¡¨ç¤º --> QKVæƒé‡çŸ©é˜µ --> QKVçŸ©é˜µ --> åˆ†å¤´è¡ŒåŠ¨Hä¸º2--> è®¡ç®—
a2 --> b2 --> d
c --> a2
```

### æ³¨æ„åŠ›æœºåˆ¶çš„åº”ç”¨

![image-20250723180947537](/assets/images/post/image-20250723180947537.png)

å›¾ç‰‡ä¸­æˆ‘æ ‡äº†ä¸‰ä¸ªåœ°æ–¹ï¼š

1. æŸ¥è¯¢æ¥è‡ªå‰ä¸€ä¸ªè§£ç å™¨ï¼Œé”®å’Œå€¼æ¥è‡ªç¼–ç å™¨çš„è¾“å‡ºï¼Œè¿™æ ·è§£ç å™¨æ¯ä¸ªä½ç½®éƒ½èƒ½å…³æ³¨è¾“å…¥åºåˆ—çš„æ‰€æœ‰ä½ç½®ã€‚æ¨¡ä»¿äº†`the typical encoder-decoder attention mechanisms in sequence-to-sequence models`
2. ç¼–ç å™¨ä¸­çš„è‡ªæ³¨æ„åŠ›å±‚ã€‚
3. è§£ç å™¨ä¸­çš„æ©ç æ³¨æ„åŠ›ã€‚å·¦ä¸‰è§’ï¼Œåªèƒ½çœ‹è§å‰é¢çš„ï¼Œçœ‹ä¸è§åé¢çš„ã€‚

> | ç‰¹æ€§     | è‡ªæ³¨æ„åŠ›                                              | ä¼ ç»Ÿæ³¨æ„åŠ›ï¼ˆå¦‚æœºå™¨ç¿»è¯‘ä¸­çš„æ³¨æ„åŠ›ï¼‰  |
> | -------- | ----------------------------------------------------- | ----------------------------------- |
> | æŸ¥è¯¢æ¥æº | ä¸é”®ã€å€¼æ¥è‡ªåŒä¸€åºåˆ—                                  | æŸ¥è¯¢æ¥è‡ªç›®æ ‡åºåˆ—ï¼Œé”® / å€¼æ¥è‡ªæºåºåˆ— |
> | æ ¸å¿ƒç›®æ ‡ | å»ºæ¨¡åºåˆ—å†…éƒ¨çš„å…³ç³»                                    | å»ºç«‹æºåºåˆ—ä¸ç›®æ ‡åºåˆ—çš„å¯¹é½          |
> | åº”ç”¨åœºæ™¯ | ç¼–ç å™¨å’Œè§£ç å™¨ï¼ˆå¦‚ Transformerï¼‰                      | è§£ç å™¨ï¼ˆå¦‚ Seq2Seq+Attentionï¼‰      |
> | å…¸å‹ä¾‹å­ | å¤„ç†å¥å­ â€œå“ˆåŸºå’ªå–œæ¬¢å—åŒ—â€ æ—¶ï¼Œè®© â€œå“ˆåŸºå’ªâ€ å…³æ³¨ â€œå—åŒ—â€ | ç¿»è¯‘ â€œæˆ‘çˆ±ä½ â€ æ—¶ï¼Œâ€œIâ€ å…³æ³¨ â€œæˆ‘â€     |

## ä½ç½®å‰åé¦ˆç½‘ç»œï¼ˆPosition-wise Feed-Forward Networksï¼‰

ç”±ä¸¤ä¸ªçº¿æ€§å˜æ¢ç»„æˆï¼Œä¸­æœ‰ä¸€ä¸ª`ReLU`æ¿€æ´»å‡½æ•°ã€‚
$$
FFN(x)=MAX(0,xW_1+b_1)W_2+b_2
$$

> å¦ä¸€ç§æè¿°æ˜¯å°†å…¶è§†ä¸ºä¸¤ä¸ªæ ¸å¤§å°ä¸º1çš„å·ç§¯ã€‚è¾“å…¥å’Œè¾“å‡ºçš„ç»´åº¦ä¸º$d_{model} =512$ï¼Œå†…å±‚ç»´åº¦ä¸º$d_{ff}=2048$ã€‚

### ReLU

$$
f(x)=max(0,x)
$$

## åµŒå…¥ï¼ˆEmbeddingsï¼‰å’ŒSoftmax

åµŒå…¥å±‚å°±æ˜¯æŠŠtokenå˜æˆç»´åº¦ä¸º$d_{model}$çš„å‘é‡ã€‚

è§£ç å±‚è¾“å‡ºä¹Ÿæ¥ä¸€ä¸ªå­¦ä¹ çš„çº¿æ€§è½¬æ¢å’Œsoftmaxå‡½æ•°æ¥è½¬æ¢ä¸ºé¢„æµ‹çš„ä¸‹ä¸€ä»¤ç‰Œæ¦‚ç‡ã€‚

ä¸¤ä¸ªåµŒå…¥å±‚å’Œsoftå‰é¢çš„çº¿æ€§å±‚æ˜¯åŒæ ·çš„æƒé‡çŸ©é˜µã€‚

åœ¨åµŒå…¥å±‚ï¼Œä»–ä»¬å°†è¿™äº›æƒé‡ä¹˜ä»¥$\sqrt{d_{model}}$â€‹â€‹.

![image-20250724104219423](/assets/images/post/image-20250724104219423.png)

> 1.ä¸ºä»€ä¹ˆåœ¨åµŒå…¥å±‚æƒé‡ä¹˜ä»¥$\sqrt{d_{model}}$ï¼Ÿ
>
> token â†’ æŸ¥è¡¨å¾— e â†’ Ã—âˆšd_model â†’ è¾“å…¥æ¨¡å‹
>
> åŸå› ï¼š
>
> - åµŒå…¥å‘é‡çš„åˆå§‹æ–¹å·®å°ï¼ˆæ¯”å¦‚ä» N(0, 1) åˆå§‹åŒ–ï¼‰
> - å¦‚æœä¸æ”¾å¤§ï¼Œ`Q @ K^T` çš„å€¼ä¼šå¤ªå° â†’ Softmax é¥±å’Œ â†’ æ¢¯åº¦æ¶ˆå¤±
> - æ‰€ä»¥æ”¾å¤§è¾“å…¥ï¼Œè®©æ³¨æ„åŠ›å¾—åˆ†åˆ«å¤ªå°
>
> 2.ä¸ºä»€ä¹ˆ pre-softmax å±‚ä¸ Ã—âˆšd_modelï¼Ÿ
>
> å› ä¸ºè¾“å…¥å·²ç» Ã—âˆšd_model äº†ï¼Œæ¨¡å‹å­¦åˆ°çš„`h`å·²ç»â€œå˜å¤§â€äº†ï¼Œå¦‚æœè¾“å‡ºå†æ”¾å¤§ï¼Œlogits ä¼šçˆ†ç‚¸ã€‚æ‰€ä»¥åªæ”¾ä¸€æ¬¡ï¼Œä¿æŒå¹³è¡¡ã€‚

```python
# å“ˆåŸºå’ª - 133
# å—åŒ— - 23
# ç»¿è±† - 58
tokens = [133, 23, 58]
```

æ¯”å¦‚æˆ‘ç»™ä¸€å¥è¯ï¼š`å“ˆåŸºå’ªå—åŒ—`

`tokenizer`è¯è¡¨å¤§å°`1000`

å¯¹åº”çš„tokenæ˜¯`133`å’Œ`23`

æ¨¡å‹ç»´åº¦æ˜¯`4`

åµŒå…¥å±‚å¤§å°æ˜¯`(1000,4)`

`133`å’Œ`23`éƒ½å˜æˆä¸€ä¸ªç»´åº¦`4`çš„å‘é‡

è¾“å…¥`(2,4)`é€šè¿‡æ³¨æ„åŠ›å±‚

è¿˜æ˜¯ç”¨ä¸Šé¢çš„æ³¨æ„åŠ›å±‚çš„é…ç½®`head=2`

$W_Q:(4\ 8)$

$W_K:(4\ 8) $

$W_V:(4\ 16)$

$W_O:(16\ 4)$

ç„¶å`(2,4)`->é€šè¿‡`softmax( Q_h(2,2,4) @ K_h^T(2,2,4) )` @ `V_h(2,2,8)`

å¾—åˆ°`(2,2,8)`æœ€å`concat`å˜æˆ`(2,16)`å’Œ$W_O$å˜å› `(2,4)`

é€šè¿‡`FFN`ï¼ˆå‡è®¾ç¬¬ä¸€å±‚`(4,16)` ç¬¬äºŒå±‚`(16,4)`ï¼‰è¿˜æ˜¯`(2,4)`

é€šè¿‡`pre-softmax`çº¿æ€§å±‚`(4,1000)`å˜æˆ`(2,1000)`ï¼Œå–æœ€åä¸€ä¸ªçš„`1000`åš`softmax`

å¾—åˆ°æ¦‚ç‡æœ€å¤§çš„`token`

### softmax

$$
Softmax(z_i)=\frac{exp(z_i)}{\textstyle \sum_{j}^{}exp(z_j) }
$$

## ä½ç½®ç¼–ç ï¼ˆPositional Encodingï¼‰

å› ä¸ºæ¨¡å‹æ²¡æœ‰å¾ªç¯å’Œå·ç§¯ï¼Œæ¨¡å‹è¦åˆ©ç”¨åºåˆ—çš„é¡ºåºï¼Œæ‰€ä»¥è¦åŠ æ ‡è®°ç›¸å¯¹æˆ–è€…ç»å¯¹ä½ç½®çš„ä¿¡æ¯ã€‚

åœ¨ç¼–ç å™¨å’Œè§£ç å™¨å †æ ˆçš„åº•éƒ¨åŠ å…¥äº†ä½ç½®ç¼–ç ï¼Œç»´åº¦å’Œ$d_{model}$å’ŒåµŒå…¥å±‚ç›¸åŒï¼Œå› æ­¤äºŒè€…å¯ä»¥ç›¸åŠ ã€‚ä½ç½®ç¼–ç æœ‰å¤šç§é€‰æ‹©ï¼ŒåŒ…å«å­¦ä¹ åˆ°çš„å’Œå›ºå®šçš„ã€‚

ä»–ä»¬ç”¨çš„æ˜¯ä¸åŒé¢‘ç‡çš„æ­£å¼¦å’Œä½™å¼¦å‡½æ•°ï¼š
$$
PE_{(pos,2i)}=sin(pos/10000^{2i/d_{model}})\\
PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{model}})
$$

- `pos`è¡¨ç¤ºä½ç½®
- `i`è¡¨ç¤ºç»´åº¦
- è¿™å…¬å¼è°æƒ³çš„ï¼ŸğŸ¤¬

****

### ä½ç½®ç¼–ç çš„è®¾è®¡

**æœŸæœ›ï¼šæ¨¡å‹èƒ½å­¦åˆ° `pos + k` å’Œ `pos` çš„å…³ç³»**

ç°åœ¨æ²¡æœ‰ä½ç½®ç¼–ç çš„è¯ï¼Œæ˜¯è¿™æ ·çš„æˆ‘ç»™ä¸¤å¥è¯`å“ˆåŸºå’ªå—åŒ—`å’Œ`å—åŒ—å“ˆåŸºå’ª`é€šè¿‡æ³¨æ„åŠ›å±‚åç»“æœè¾“å‡ºçš„å€¼æ˜¯ä¸€æ ·çš„ï¼Œæ‰€ä»¥å°±å¾—ç»™ä»–åšç‚¹æ‰‹è„šæ ‡è¯†tokençš„å‰åé¡ºåºã€‚

#### ç»å¯¹ä½ç½®ç¼–ç 

æˆ‘ç›´æ¥æŒ‰ç…§é¡ºåºç»™ä»–åŠ å°±å®Œäº‹äº†å‘—

æ¯”å¦‚``# å“ˆåŸºå’ª - 133`å’Œ `# å—åŒ— - 23`çš„å‘é‡è¡¨ç¤ºåˆ†åˆ«ä¸º`[1,2]`å’Œ`[2,3]`

ä½ç½®ç¼–ç æ˜¯`[0,0]`å’Œ`[1,1]`

æˆ‘ç›´æ¥å°±æ˜¯å‘é‡+ä½ç½®å˜æˆ`[1,2]`å’Œ`[3,4]`

##### é—®é¢˜

1. **æ•°å€¼èŒƒå›´ä¸å‡è¡¡** ï¼š
   - è¯å‘é‡ `[1,2]` æ•°å€¼å°
   - ä½ç½®ç¼–ç  `[100,100]`ï¼ˆç¬¬100ä¸ªè¯ï¼‰æ•°å€¼å¤§
   - ç›¸åŠ åï¼Œä½ç½®ä¿¡æ¯â€œæ·¹æ²¡â€äº†è¯ä¿¡æ¯ï¼Œæˆ–è€…åè¿‡æ¥ã€‚
2. **æ— æ³•å¤–æ¨ï¼ˆOut-of-Domainï¼‰** ï¼š
   - å¦‚æœè®­ç»ƒæ—¶æœ€é•¿åºåˆ—æ˜¯ 512ï¼Œæµ‹è¯•æ—¶æ¥äº†ä¸ª 600 ä¸ªè¯çš„å¥å­ï¼Œä½ çš„ `[600,600]` å¯èƒ½ä»æ¥æ²¡å­¦è¿‡ï¼Œè¡¨ç°ä¼šå´©ã€‚
3. **æ²¡æœ‰å‘¨æœŸæ€§/å¹³æ»‘æ€§** ï¼š
   - ç†æƒ³çš„ä½ç½®ç¼–ç åº”è¯¥è®©â€œç›¸é‚»ä½ç½®â€çš„ç¼–ç ä¹Ÿç›¸ä¼¼ï¼ˆå¹³æ»‘è¿‡æ¸¡ï¼‰ã€‚

#### å¯å­¦ä¹ çš„ä½ç½®ç¼–ç 

BERTã€GPT ç­‰æ¨¡å‹å®é™…ç”¨çš„æ–¹å¼

```python
import torch
import torch.nn as nn

# å‡è®¾è¯è¡¨å¤§å° 1000ï¼Œembedding ç»´åº¦ 128ï¼Œæœ€å¤§åºåˆ—é•¿åº¦ 512
vocab_size = 1000
d_model = 128
max_len = 512

# è¯åµŒå…¥
word_embed = nn.Embedding(vocab_size, d_model)

# ä½ç½®åµŒå…¥ï¼ˆå¯å­¦ä¹ ï¼ï¼‰
pos_embed = nn.Embedding(max_len, d_model)

# è¾“å…¥åºåˆ—ï¼šæ¯”å¦‚ [0, 1, 2] è¡¨ç¤ºä¸‰ä¸ª token çš„ id
input_ids = torch.tensor([0, 1, 2])  # å“ˆã€åŸºã€å’ª

# ä½ç½®ç´¢å¼•ï¼š[0, 1, 2]
positions = torch.arange(input_ids.size(0))

# è·å–åµŒå…¥
word_emb = word_embed(input_ids)        # [3, 128]
pos_emb = pos_embed(positions)          # [3, 128]

# ç›¸åŠ ï¼
final_emb = word_emb + pos_emb          # [3, 128]
```

----

#### ä½ç½®ç¼–ç å‡½æ•°

è¿˜æ˜¯è¿™ä¸ªä¾‹å­ï¼š

```python
# å“ˆåŸºå’ª - 133
# å—åŒ— - 23
# ç»¿è±† - 58

# å“ˆåŸºå’ªå—åŒ—ç»¿è±†
tokens = [133, 23, 58]
# å—åŒ—å“ˆåŸºå’ªç»¿è±†
tokens = [23, 133, 58]
# æ²¡æœ‰ä½ç½®ç¼–ç ï¼Œå¯¼è‡´æ¨¡å‹è®¤ä¸ºè¿™ä¸¤æ˜¯ä¸€æ ·çš„
```

æ¥ä¸€å¥è¯ï¼š`å“ˆåŸºå’ª å—åŒ— ç»¿è±† é˜¿å˜è¥¿ å“ˆåŸºå’ª æ›¼æ³¢ å—åŒ— ç»¿è±†`ä¸€å…± 8 ä¸ª tokenï¼Œä½ç½®ä» 0 åˆ° 7ã€‚

- ç¬¬ä¸€ä¸ªâ€œå“ˆåŸºå’ªâ€åœ¨ **ä½ç½® 0**
- ç¬¬äºŒä¸ªâ€œå“ˆåŸºå’ªâ€åœ¨ **ä½ç½® 4**

`d_model = 4`

ä½ç½® 0 (`pos=0`) çš„ä½ç½®ç¼–ç ï¼š

- `i=0` (å¶æ•°ç»´)ï¼šsin(0/100000/4)=sin(0)=0
- `i=0` (å¥‡æ•°ç»´)ï¼šcos(0/100000/4)=cos(0)=1
- `i=1` (å¶æ•°ç»´)ï¼šsin(0/100002/4)=sin(0/100)=sin(0)=0
- `i=1` (å¥‡æ•°ç»´)ï¼šcos(0/100)=cos(0)=1

æ‰€ä»¥ï¼š
$$
PE_0=[0,1,0,1]
$$
ä½ç½® 4 (`pos=4`) çš„ä½ç½®ç¼–ç ï¼š

- `i=0` (å¶æ•°ç»´)ï¼šsin(4/1)=sin(4)â‰ˆsin(229âˆ˜)â‰ˆâˆ’0.7568
- `i=0` (å¥‡æ•°ç»´)ï¼šcos(4)â‰ˆcos(229âˆ˜)â‰ˆâˆ’0.6536
- `i=1` (å¶æ•°ç»´)ï¼šsin(4/100)=sin(0.04)â‰ˆ0.03999
- `i=1` (å¥‡æ•°ç»´)ï¼šcos(0.04)â‰ˆ0.9992

æ‰€ä»¥ï¼š
$$
PE_4â‰ˆ[âˆ’0.7568,âˆ’0.6536,0.03999,0.9992]
$$
å‡è®¾`å“ˆåŸºå’ª`çš„è¯å‘é‡æ˜¯ `[1.0, 0.5, 0.8, 0.3]`ï¼ˆéšä¾¿ç¼–çš„ï¼‰

ç¬¬ä¸€ä¸ª`å“ˆåŸºå’ª`ï¼ˆä½ç½® 0ï¼‰ï¼š
$$
Input_0=[1.0,0.5,0.8,0.3]+[0,1,0,1]=[1.0,1.5,0.8,1.3]
$$
ç¬¬äºŒä¸ª`å“ˆåŸºå’ª`ï¼ˆä½ç½® 4ï¼‰ï¼š
$$
Input_4=[1.0,0.5,0.8,0.3]+[âˆ’0.7568,âˆ’0.6536,0.03999,0.9992]â‰ˆ[0.2432,âˆ’0.1536,0.83999,1.2992]
$$
ç„¶åå°±æ˜¯çœ‹è¿™ä¸ªå‡½æ•°å›¾åƒ

```python
import numpy as np
import matplotlib.pyplot as plt

def get_positional_encoding(max_len, d_model):
    # åˆ›å»ºä¸€ä¸ªè¶³å¤Ÿå¤§çš„çŸ©é˜µ
    pe = np.zeros((max_len, d_model))
    
    # åˆ›å»ºä½ç½®åˆ—å‘é‡: [max_len, 1]
    position = np.expand_dims(np.arange(0, max_len), 1)
    
    # åˆ›å»ºåˆ†æ¯é¡¹: 10000^(2i/d_model)
    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))
    
    # åº”ç”¨ sin å’Œ cos
    pe[:, 0::2] = np.sin(position * div_term)  # å¶æ•°ç»´
    pe[:, 1::2] = np.cos(position * div_term)  # å¥‡æ•°ç»´
    
    # æ·»åŠ  batch ç»´åº¦ (å¯é€‰): [1, max_len, d_model]
    pe = np.expand_dims(pe, axis=0)
    
    return pe

# ç¤ºä¾‹ï¼šç”Ÿæˆé•¿åº¦ä¸º 50ï¼Œç»´åº¦ä¸º 512 çš„ä½ç½®ç¼–ç 
max_len = 50
d_model = 512
pe = get_positional_encoding(max_len, d_model)

print("Positional encoding shape:", pe.shape)  # (1, 50, 512)

# å¯è§†åŒ–å‰å‡ ä¸ªç»´åº¦
plt.figure(figsize=(12, 6))
plt.pcolormesh(pe[0], cmap='RdBu')
plt.xlabel('Embedding Dimension')
plt.ylabel('Position')
plt.title('Positional Encoding')
plt.colorbar()
plt.show()
```

![image-20250724170440852](/assets/images/post/image-20250724170440852.png)

å›¾é‡Œé¢æ¯ä¸€è¡Œå°±æ˜¯ä¸€ä¸ªtokençš„ä½ç½®ç¼–ç å‘é‡è¡¨ç¤ºã€‚

1. **å·¦è¾¹ï¼ˆä½ç»´ï¼‰ï¼šå˜åŒ–å¿« â†’ é«˜é¢‘**

   - é è¿‘ X=0 çš„åŒºåŸŸï¼ˆå‰å‡ åˆ—ï¼‰ï¼Œé¢œè‰²ä¸Šä¸‹è·³åŠ¨éå¸¸å‰§çƒˆã€‚

   - æ¯éš”å‡ ä¸ªä½ç½®å°±ä»çº¢å˜è“ï¼Œè¯´æ˜è¿™äº›ç»´åº¦å¯¹**ç›¸é‚»ä½ç½®éå¸¸æ•æ„Ÿ** ã€‚

â€‹	ğŸ‘‰ è¿™äº›æ˜¯ **é«˜é¢‘ç»´åº¦** ï¼Œç”¨æ¥åŒºåˆ†â€œç¬¬3ä¸ªè¯â€å’Œâ€œç¬¬4ä¸ªè¯â€è¿™ç§ç»†å¾®å·®åˆ«ã€‚

2. **å³è¾¹ï¼ˆé«˜ç»´ï¼‰ï¼šå˜åŒ–æ…¢ â†’ ä½é¢‘**

   - é è¿‘ X=512 çš„åŒºåŸŸï¼ˆåå‡ åˆ—ï¼‰ï¼Œé¢œè‰²å˜åŒ–éå¸¸å¹³ç¼“ï¼Œä¸€å¤§ç‰‡çº¢è‰²æˆ–è“è‰²ã€‚

   - å¯èƒ½è¦åˆ° Y=40 æ‰å¼€å§‹ä»çº¢å˜ç™½å†å˜è“ã€‚

â€‹	ğŸ‘‰ è¿™äº›æ˜¯ **ä½é¢‘ç»´åº¦** ï¼Œç”¨æ¥è¡¨ç¤ºâ€œè¿™æ˜¯åºåˆ—çš„å¼€å¤´â€è¿˜æ˜¯â€œè¿™æ˜¯åºåˆ—çš„ç»“å°¾â€ã€‚

3. **æ•´ä½“åƒâ€œæ¡çº¹ç”»â€æˆ–â€œå¹²æ¶‰å›¾æ ·â€**

   - ä¸åŒé¢‘ç‡çš„æ³¢å åŠ åœ¨ä¸€èµ·ï¼Œå½¢æˆäº†å¤æ‚çš„çº¹ç†ã€‚

   - æ²¡æœ‰ä¸¤ä¸ªä½ç½®çš„â€œæ¡çº¹æ¨¡å¼â€æ˜¯å®Œå…¨ä¸€æ ·çš„ã€‚

> è®ºæ–‡å°±è¿™å‡ è¡Œå­—ï¼Œæˆ‘æé¦¬å®é™…ç†è§£èµ·æ¥å¾—æœå¤šå°‘çœ‹å•Šï¼Œè¿™å—çœŸæœ‰ç‚¹éš¾æå§
>
> è®ºï¼š*è‹å‰‘æ—ä¸ºä»€ä¹ˆæ˜¯ç¥ï¼Ÿç¥ä¸ºä»€ä¹ˆæ˜¯è‹å‰‘æ—ï¼Ÿ*
>
> æœ€åè´´ä¸€ä¸‹è‹ç¥çš„è§£é‡Šï¼ˆæ•°å­¦å¾—å­¦å•Šï¼‰ï¼šhttps://kexue.fm/archives/8231

## Why Self-Attention

æ‹¿æ³¨æ„åŠ›ä¸å¾ªç¯å±‚å’Œå·ç§¯å±‚å¯¹æ¯”ï¼Œè§å›¾ï¼š

![image-20250724172742581](/assets/images/post/image-20250724172742581.png)

## è®­ç»ƒï¼ˆTrainingï¼‰

### ä¼˜åŒ–å™¨ï¼ˆOptimizerï¼‰

#### SGD

éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆStochastic Gradient Descentï¼‰ï¼Œ**æ¯æ¬¡è¿­ä»£åªéšæœºä½¿ç”¨ä¸€ä¸ªæ ·æœ¬ï¼ˆæˆ–ä¸€ä¸ªå°æ‰¹é‡æ ·æœ¬ï¼‰æ¥è®¡ç®—æ¢¯åº¦å¹¶æ›´æ–°å‚æ•°**ï¼Œè€Œä¸æ˜¯åƒä¼ ç»Ÿæ¢¯åº¦ä¸‹é™é‚£æ ·æ¯æ¬¡ä½¿ç”¨å…¨éƒ¨æ•°æ®ï¼ˆæ‰¹é‡æ¢¯åº¦ä¸‹é™ï¼‰ã€‚

ç®€å•çš„å®ç°ï¼š

```python
class MiniSGD:
    def __init__(self, params, lr):
        self.params = list(params)   # æŠŠ wã€b å­˜è¿›æ¥
        self.lr = lr                 # å­¦ä¹ ç‡

    def zero_grad(self):
        for p in self.params:
            if p.grad is not None:
                p.grad.zero_()       # æŠŠæ¢¯åº¦æ¸…é›¶

    def step(self):
        for p in self.params:
            if p.grad is not None:
                p.data -= self.lr * p.grad   # å…³é”®æ›´æ–°ï¼

                             
import torch

# 1. é€  5 ä¸ªæ•°æ®ç‚¹ï¼ˆx, yï¼‰
x = torch.tensor([0.0, 1.0, 2.0, 3.0, 4.0]).reshape(-1, 1)
y = 2 * x + 1           # çœŸå®ç›´çº¿ y = 2x + 1

# 2. éšæœºåˆå§‹åŒ– wï¼ˆæ–œç‡ï¼‰å’Œ bï¼ˆæˆªè·ï¼‰
w = torch.tensor([0.0], requires_grad=True)   # åˆå§‹ççŒœæ–œç‡=0
b = torch.tensor([0.0], requires_grad=True)   # åˆå§‹ççŒœæˆªè·=0

# 3. è®¾ç½®ä¼˜åŒ–å™¨ï¼šSGDï¼Œå­¦ä¹ ç‡=0.01
# optimizer = torch.optim.SGD([w, b], lr=0.01)
optimizer = MiniSGD([w, b], lr=0.01)

# 4. è®­ç»ƒ 100 æ¬¡
for step in range(100):
    pred = w * x + b          # å½“å‰ç›´çº¿
    loss = ((pred - y) ** 2).mean()  # å‡æ–¹è¯¯å·®
    
    optimizer.zero_grad()     # æ¸…ç©ºä¸Šä¸€æ­¥çš„æ¢¯åº¦
    loss.backward()           # è®¡ç®—æ¢¯åº¦
    optimizer.step()          # ç”¨ SGD æ›´æ–° w å’Œ b
    
    if step % 20 == 0:        # æ¯20æ¬¡æ‰“å°ä¸€æ¬¡
        print(f"ç¬¬{step:3d}æ­¥: w={w.item():.2f}, b={b.item():.2f}, loss={loss.item():.3f}")
        
# ç¬¬  0æ­¥: w=0.28, b=0.10, loss=33.000
# ç¬¬ 20æ­¥: w=1.98, b=0.73, loss=0.123
# ç¬¬ 40æ­¥: w=2.07, b=0.79, loss=0.015
# ç¬¬ 60æ­¥: w=2.06, b=0.82, loss=0.012
# ç¬¬ 80æ­¥: w=2.06, b=0.84, loss=0.009
```

**ä¼˜ç‚¹**ï¼š

- **é«˜æ•ˆ**ï¼šå°¤å…¶é€‚åˆå¤§æ•°æ®é›†ï¼ˆå¦‚æ·±åº¦å­¦ä¹ ï¼‰ï¼Œæ— éœ€æ¯æ¬¡éå†å…¨éƒ¨æ•°æ®ã€‚
- **å†…å­˜å‹å¥½**ï¼šå¯åœ¨çº¿å­¦ä¹ ï¼ˆé€æ ·æœ¬æ›´æ–°ï¼‰ã€‚
- **å¯èƒ½è·³å‡ºå±€éƒ¨æœ€å°å€¼**ï¼šå™ªå£°æœ‰åŠ©äºé€ƒç¦»å°–é”çš„å±€éƒ¨æå°å€¼ã€‚

**ç¼ºç‚¹**ï¼š

- **éœ‡è¡å‰§çƒˆ**ï¼šæ”¶æ•›è·¯å¾„ä¸ç¨³å®šï¼Œå¯èƒ½éš¾ä»¥è¾¾åˆ°ç²¾ç¡®çš„æœ€ä¼˜ç‚¹ã€‚
- **éœ€è°ƒæ•´å­¦ä¹ ç‡**ï¼šå­¦ä¹ ç‡è¿‡å¤§å¯èƒ½å‘æ•£ï¼Œè¿‡å°åˆ™æ”¶æ•›æ…¢ã€‚
- **å¯¹ç‰¹å¾ç¼©æ”¾æ•æ„Ÿ**ï¼šéœ€æ ‡å‡†åŒ–æ•°æ®ã€‚

**æ”¹è¿›**ï¼š

- **åŠ¨é‡ï¼ˆMomentumï¼‰**ï¼šå¼•å…¥æƒ¯æ€§ï¼ŒåŠ é€Ÿæ”¶æ•›å¹¶å‡å°‘éœ‡è¡ã€‚
- **AdaGrad**ï¼šè‡ªé€‚åº”è°ƒæ•´æ¯ä¸ªå‚æ•°çš„å­¦ä¹ ç‡ã€‚
- **RMSProp**ï¼šè§£å†³`AdaGrad`å­¦ä¹ ç‡è¿‡å¿«ä¸‹é™çš„é—®é¢˜ã€‚
- **Adam**ï¼šç»“åˆåŠ¨é‡å’Œ`RMSProp`ï¼Œæœ€å¸¸ç”¨çš„ä¼˜åŒ–å™¨ä¹‹ä¸€ã€‚

#### SGD_Momentum

`å¸¦åŠ¨é‡çš„éšæœºæ¢¯åº¦ä¸‹é™`æ˜¯ éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆStochastic Gradient Descent, SGDï¼‰çš„ä¸€ç§æ”¹è¿›ç‰ˆæœ¬ï¼Œé€šè¿‡å¼•å…¥â€œåŠ¨é‡ï¼ˆMomentumï¼‰â€æœºåˆ¶æ¥åŠ é€Ÿæ”¶æ•›å¹¶å‡å°‘è®­ç»ƒè¿‡ç¨‹ä¸­çš„éœ‡è¡ã€‚

**åŠ¨é‡ï¼ˆMomentumï¼‰** çš„æ€æƒ³æ˜¯ï¼š**æ¨¡æ‹Ÿç‰©ç†ä¸­çš„åŠ¨é‡** ï¼Œè®©å‚æ•°æ›´æ–°å…·æœ‰â€œæƒ¯æ€§â€â€”â€”å¦‚æœæ¢¯åº¦æ–¹å‘ä¸€è‡´ï¼Œå°±åŠ é€Ÿå‰è¿›ï¼›å¦‚æœæ–¹å‘å˜åŒ–å¤§ï¼Œå°±å¹³æ»‘éœ‡è¡ã€‚

```python
class MiniSGD_Momentum:
    def __init__(self, params, lr=0.01, momentum=0.9):
        self.params = list(params)
        self.lr = lr
        self.momentum = momentum
        self.v = [torch.zeros_like(p) for p in self.params]  # é€Ÿåº¦è¡¨

    def zero_grad(self):
        for p in self.params:
            if p.grad is not None:
                p.grad.zero_()

    def step(self):
        for p, v in zip(self.params, self.v):
            if p.grad is not None:
                v.mul_(self.momentum).add_(p.grad)      # é€Ÿåº¦ = æ—§é€Ÿåº¦*0.9 + æ–°æ¢¯åº¦
                p.data -= self.lr * v                   # èµ°è¿™ä¸€æ­¥
```

#### AdaGrad

åœ¨ä¼ ç»Ÿçš„ SGD æˆ– Momentum ä¸­ï¼Œ**æ‰€æœ‰å‚æ•°ä½¿ç”¨ç›¸åŒçš„å­¦ä¹ ç‡** ã€‚ä½†åœ¨å®é™…ä¸­ï¼š

- æœ‰äº›å‚æ•°æ›´æ–°é¢‘ç¹ï¼Œå¯èƒ½éœ€è¦è¾ƒå°çš„å­¦ä¹ ç‡ï¼›
- æœ‰äº›å‚æ•°æ›´æ–°ç¨€ç–ï¼ˆå¦‚ NLP ä¸­çš„ç½•è§è¯ embeddingï¼‰ï¼Œå¯èƒ½éœ€è¦æ›´å¤§çš„å­¦ä¹ ç‡ã€‚

**AdaGrad çš„è§£å†³æ–¹æ¡ˆ** ï¼š

> **ä¸ºæ¯ä¸ªå‚æ•°ç»´æŠ¤ä¸€ä¸ªç‹¬ç«‹çš„å­¦ä¹ ç‡** ï¼Œæ ¹æ®å…¶å†å²æ¢¯åº¦çš„ç´¯ç§¯å¤§å°è‡ªåŠ¨è°ƒæ•´â€”â€”**æ¢¯åº¦å¤§çš„å‚æ•°ï¼Œå­¦ä¹ ç‡è‡ªåŠ¨å˜å°ï¼›æ¢¯åº¦å°æˆ–ç¨€ç–çš„å‚æ•°ï¼Œå­¦ä¹ ç‡ä¿æŒè¾ƒå¤§ã€‚** 

```python
class MiniAdaGrad:
    def __init__(self, params, lr=0.01):
        self.params = list(params)
        self.lr = lr
        self.G = [torch.zeros_like(p) for p in self.params]  # å†å²æ¢¯åº¦å¹³æ–¹å’Œè¡¨

    def zero_grad(self):
        for p in self.params:
            if p.grad is not None:
                p.grad.zero_()

    def step(self):
        for p, g in zip(self.params, self.G):
            if p.grad is not None:
                g.add_(p.grad ** 2)                             # ç´¯åŠ å¹³æ–¹
                p.data -= self.lr * p.grad / (g.sqrt() + 1e-8)  # å­¦ä¹ ç‡éšå†å²è‡ªåŠ¨å˜å°
```

**ä¼˜ç‚¹**

1. **è‡ªé€‚åº”å­¦ä¹ ç‡**
   - æ¯ä¸ªå‚æ•°æœ‰è‡ªå·±çš„å­¦ä¹ ç‡
   - é¢‘ç¹æ›´æ–°çš„å‚æ•° â†’ æ¢¯åº¦å¹³æ–¹å’Œå¤§ â†’ å­¦ä¹ ç‡è‡ªåŠ¨ä¸‹é™
   - ç¨€ç–æ›´æ–°çš„å‚æ•° â†’ æ¢¯åº¦å¹³æ–¹å’Œå° â†’ å­¦ä¹ ç‡ä¿æŒè¾ƒå¤§
2. **éå¸¸é€‚åˆç¨€ç–æ•°æ®**
   - å¦‚è¯åµŒå…¥è®­ç»ƒä¸­ï¼ŒæŸäº›è¯å¾ˆå°‘å‡ºç°ï¼ŒAdaGrad èƒ½ç»™å®ƒä»¬æ›´å¤§çš„æ›´æ–°æ­¥é•¿ï¼Œæœ‰åŠ©äºå­¦ä¹ 
3. **æ— éœ€æ‰‹åŠ¨ä¸ºä¸åŒå‚æ•°è°ƒå­¦ä¹ ç‡**
   - è‡ªåŠ¨è°ƒèŠ‚ï¼Œç®€åŒ–è°ƒå‚è¿‡ç¨‹

**ç¼ºç‚¹**

1. **å­¦ä¹ ç‡å•è°ƒé€’å‡**
   - gæ˜¯ä¸æ–­ç´¯åŠ çš„ï¼Œåªä¼šå¢å¤§ä¸ä¼šå‡å°
   - å¯¼è‡´å­¦ä¹ ç‡æŒç»­ä¸‹é™ï¼Œæœ€ç»ˆè¶‹è¿‘äº 0
   - **åæœ** ï¼šè®­ç»ƒåæœŸå‚æ•°å‡ ä¹ä¸å†æ›´æ–°ï¼Œå¯èƒ½æå‰æ”¶æ•›åˆ°æ¬¡ä¼˜ç‚¹
2. **å¯¹éç¨€ç–é—®é¢˜æ•ˆæœä¸€èˆ¬**
   - åœ¨å›¾åƒã€è¯­éŸ³ç­‰å¯†é›†æ¢¯åº¦ä»»åŠ¡ä¸­ï¼Œå®¹æ˜“è¿‡æ—©é™ä½å­¦ä¹ ç‡

#### RMSProp

å›é¡¾ä¸€ä¸‹ **AdaGrad çš„é—®é¢˜** ï¼š

> âŒ **å†å²æ¢¯åº¦å¹³æ–¹æ˜¯ç´¯åŠ çš„
> â†’ å¯¼è‡´è¶Šæ¥è¶Šå¤§ â†’ å­¦ä¹ ç‡æŒç»­ä¸‹é™ â†’ åæœŸå‡ ä¹ä¸æ›´æ–°å‚æ•° 

**RMSProp çš„æ”¹è¿›æ€è·¯** ï¼š

> âœ… ä¸å†ç®€å•ç´¯åŠ æ‰€æœ‰å†å²æ¢¯åº¦å¹³æ–¹ï¼Œè€Œæ˜¯ä½¿ç”¨**æŒ‡æ•°åŠ æƒç§»åŠ¨å¹³å‡ï¼ˆExponentially Weighted Moving Averageï¼‰** æ¥è®¡ç®—å½“å‰æ¢¯åº¦çš„â€œè¿‘æœŸâ€å¹³æ–¹å‡å€¼ã€‚ 

è¿™æ ·å¯ä»¥ï¼š

- å¿˜è®°å¤ªä¹…è¿œçš„å†å²
- è®©å­¦ä¹ ç‡ä¸ä¼šæ— é™ä¸‹é™
- æ›´å¥½åœ°é€‚åº”éå¹³ç¨³ç›®æ ‡å‡½æ•°ï¼ˆå¦‚ RNNï¼‰

```python
class MiniRMSProp:
    def __init__(self, params, lr=0.01, alpha=0.9):
        self.params = list(params)
        self.lr = lr
        self.alpha = alpha
        self.v = [torch.zeros_like(p) for p in self.params]  # æ»‘åŠ¨å¹³æ–¹å¹³å‡è¡¨

    def zero_grad(self):
        for p in self.params:
            if p.grad is not None:
                p.grad.zero_()

    def step(self):
        for p, v in zip(self.params, self.v):
            if p.grad is not None:
                v.mul_(self.alpha).addcmul_(p.grad, p.grad, value=1 - self.alpha)  # æ»‘åŠ¨å¹³å‡ v * alpha + (1-alpha) * grad^2
                p.data -= self.lr * p.grad / (v.sqrt() + 1e-8)                    # æ­¥å­å¤§å°è¢«å¹³æ»‘
```

#### Adam

Adam åŒæ—¶ä¼°è®¡æ¢¯åº¦çš„ï¼š

1. **ä¸€é˜¶çŸ©ï¼ˆå‡å€¼ï¼‰** â†’ ç±»ä¼¼ **åŠ¨é‡ï¼ˆMomentumï¼‰**
2. **äºŒé˜¶çŸ©ï¼ˆæœªä¸­å¿ƒåŒ–çš„æ–¹å·®ï¼‰** â†’ ç±»ä¼¼ **RMSProp çš„æ¢¯åº¦å¹³æ–¹å¹³å‡**

ç„¶åå¯¹è¿™ä¸¤ä¸ªçŸ©è¿›è¡Œ**åå·®æ ¡æ­£ï¼ˆbias correctionï¼‰** ï¼Œå› ä¸ºåˆå§‹ä¸º 0 ä¼šå¯¼è‡´æ—©æœŸä¼°è®¡åå°ã€‚

```python
class MiniAdam:
    def __init__(self, params, lr=0.01, beta1=0.9, beta2=0.999):
        self.params = list(params)
        self.lr = lr
        self.beta1, self.beta2 = beta1, beta2
        self.m = [torch.zeros_like(p) for p in self.params]  # åŠ¨é‡
        self.v = [torch.zeros_like(p) for p in self.params]  # äºŒé˜¶åŠ¨é‡
        self.t = 0                                           # æ­¥æ•°

    def zero_grad(self):
        for p in self.params:
            if p.grad is not None:
                p.grad.zero_()

    def step(self):
        self.t += 1
        for p, m, v in zip(self.params, self.m, self.v):
            if p.grad is not None:
                m.mul_(self.beta1).add_(p.grad, alpha=1 - self.beta1)          # åŠ¨é‡æ›´æ–° 0.9 * m + (1- 0.9) * grad
                v.mul_(self.beta2).addcmul_(p.grad, p.grad, value=1 - self.beta2)  # äºŒé˜¶åŠ¨é‡ 0.999 * v + (1-0.999) * grad^2
                # æ ¡æ­£åå·® å› ä¸ºåˆå§‹ä¸º 0ï¼Œæ—©æœŸä¼°è®¡åå°
                m_hat = m / (1 - self.beta1 ** self.t)
                v_hat = v / (1 - self.beta2 ** self.t)
                p.data -= self.lr * m_hat / (v_hat.sqrt() + 1e-8)              # æœ€ç»ˆæ­¥å­
```

### æ­£åˆ™åŒ–ï¼ˆRegularizationï¼‰

`We employ three types of regularization during training`

**ä¸æ˜¯å“¥ä»¬ä¸‰ç§åªå†™ä¸¤ä¸ªå—ï¼Ÿ**

> GTP4å›ç­”ï¼š
>
> é‚£ç¬¬ä¸‰ç§ regularization åˆ°åº•æ˜¯ä»€ä¹ˆï¼Ÿ
>
> è™½ç„¶è®ºæ–‡æ²¡æ˜ç¡®åˆ—å‡ºæ¥ï¼Œä½†æ ¹æ®ä»£ç å®ç°å’Œä½œè€…åœ¨å…¶ä»–åœºåˆçš„è®¨è®ºï¼Œ**ç¬¬ä¸‰ç§ regularization è¢«å¹¿æ³›è®¤ä¸ºæ˜¯ï¼š**
>
> > **Dropout on attention weights or feedforward layers.**
>
> 
>
> æˆ‘ä»¬å¯ä»¥æ›´æ˜ç¡®åœ°æ‹†åˆ†å¦‚ä¸‹ï¼š
>
> 1. **Residual Dropout**
>
>    - è¿™æ˜¯åœ¨æ®‹å·®è¿æ¥åçš„ dropoutã€‚
>
>    - å³ï¼š`LayerNorm(x + Dropout(SubLayer(x)))`ã€‚
>
> 2. **Label Smoothing**
>    - åœ¨è®¡ç®—äº¤å‰ç†µæŸå¤±æ—¶ï¼Œä½¿ç”¨å¹³æ»‘è¿‡çš„æ ‡ç­¾ï¼ˆæ¯”å¦‚ 0.9/0.1 è€Œä¸æ˜¯ one-hotï¼‰ã€‚
>
> 3. **Dropout inside layers**
>    - åŒ…æ‹¬ï¼š
>      - Attention weights ä¸Šçš„ dropoutï¼ˆsoftmax(QK^T/âˆšd_k) ä¹‹åï¼‰
>      - Feedforward å±‚ä¸­çš„ dropoutï¼ˆåœ¨ ReLU æ¿€æ´»ä¹‹åï¼‰
>
> åœ¨åŸå§‹ Tensor2Tensor æˆ–åæ¥çš„ PyTorch å®ç°ä¸­ï¼Œè¿™äº› dropout çš„åº”ç”¨æ˜¯åˆ†å¼€çš„ã€‚
>
> ------
>
> **æ€»ç»“**
>
> è®ºæ–‡ä¸­â€œè¯´äº†ä¸‰ç§åªå†™äº†ä¸¤ç§â€çš„ç¬¬ä¸‰ç§ regularizationï¼Œ**å®é™…ä¸Šæ˜¯æŒ‡åœ¨ attention å’Œ feedforward ç½‘ç»œå†…éƒ¨ä¹ŸåŠ äº† dropout**ï¼Œè™½ç„¶ä½œè€…æ²¡æœ‰æ˜¾å¼åˆ—å‡ºè¿™ä¸€ç‚¹ã€‚
>
> ä½ å¯ä»¥æŠŠè¿™ä¸‰ç§ç†è§£ä¸ºï¼š
>
> 1. **Dropout on residual connections**
> 2. **Dropout inside attention / feedforward layers**
> 3. **Label smoothing**

#### æ®‹å·®ä¸¢å¼ƒï¼ˆResidual Dropoutï¼‰

> We apply dropout to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of $P_{drop} = 0.1$.

```python
# ä¼ªä»£ç ï¼ˆPyTorch é£æ ¼ï¼‰
attn_output = MultiHeadAttention(x)  # å­å±‚
attn_output = Dropout(attn_output)   # Residual Dropout
x = x + attn_output                  # æ®‹å·®è¿æ¥
x = LayerNorm(x)

ff_output = FeedForward(x)
ff_output = Dropout(ff_output)
x = x + ff_output
x = LayerNorm(x)
```

Residual Dropout çš„ä½œç”¨æ˜¯ï¼š

- å¢å¼ºæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼›
- é˜²æ­¢æ¨¡å‹è®­ç»ƒæ—¶é™·å…¥å¯¹æŸä¸ªå­æ¨¡å—çš„è¿‡æ‹Ÿåˆï¼›
- åœ¨æ®‹å·®ç»“æ„ä¸­åŠ å…¥éšæœºæ€§ï¼Œæœ‰åŠ©äºæ·±å±‚æ¨¡å‹æ”¶æ•›ã€‚

| Dropout ç±»å‹        | ä½œç”¨ä½ç½®              | ä½œç”¨                 |
| ------------------- | --------------------- | -------------------- |
| Residual Dropout    | å­å±‚è¾“å‡ºï¼Œæ®‹å·®è¿æ¥å‰  | æ­£åˆ™åŒ–æ®‹å·®è·¯å¾„       |
| Attention Dropout   | `Softmax(QK^T/âˆšd)` å | éšæœºå±è”½éƒ¨åˆ†æ³¨æ„åŠ›   |
| Feedforward Dropout | FFN ä¸­ `ReLU(W1x)` å | å¸¸è§„ç¥ç»ç½‘ç»œ dropout |

#### æ ‡ç­¾å¹³æ»‘ï¼ˆLabel Smoothingï¼‰

> During training, we employed label smoothing of value $Ïµ_{ls} = 0.14$ . This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.



```python
import torch
import torch.nn.functional as F

def label_smoothing(targets, num_classes, epsilon=0.1):
    """
    æ‰‹åŠ¨å®ç° label smoothing
    - targets: LongTensor, shape (batch_size,)
    - num_classes: ç±»åˆ«æ•°
    - epsilon: å¹³æ»‘ç³»æ•°
    è¿”å›å¹³æ»‘åçš„æ ‡ç­¾ï¼ˆFloatTensorï¼‰
    """
    batch_size = targets.size(0)
    # åˆ›å»ºå…¨ä¸º epsilon/K çš„å¹³æ»‘æ ‡ç­¾
    smooth_labels = torch.full(size=(batch_size, num_classes), fill_value=epsilon / (num_classes - 1))
    # è®¾ç½®çœŸå®æ ‡ç­¾çš„æ¦‚ç‡ä¸º 1 - epsilon
    smooth_labels.scatter_(1, targets.unsqueeze(1), 1.0 - epsilon)
    return smooth_labels

# å‡è®¾æˆ‘ä»¬æœ‰3ä¸ªç±»ï¼ˆ0, 1, 2ï¼‰
num_classes = 3
batch_size = 2

# æ¨¡æ‹Ÿ logits è¾“å‡ºï¼ˆæœªç»è¿‡ softmaxï¼‰
logits = torch.tensor([[2.0, 1.0, 0.1],
                       [0.5, 2.5, 0.3]])  # shape: [2, 3]

# çœŸå®æ ‡ç­¾ï¼ˆone-hot åŸå§‹æ˜¯ [1, 0, 0] å’Œ [0, 1, 0]ï¼‰
targets = torch.tensor([0, 1])  # shape: [2]

# è·å–å¹³æ»‘åçš„æ ‡ç­¾
smoothed_targets = label_smoothing(targets, num_classes, epsilon=0.1)

# ä½¿ç”¨ KLDivLoss ä½œä¸ºäº¤å‰ç†µçš„æ›¿ä»£
# æ³¨æ„ï¼šlog_probs éœ€è¦æ˜¯ log_softmax
log_probs = F.log_softmax(logits, dim=-1)
loss = F.kl_div(log_probs, smoothed_targets, reduction='batchmean')

print("Smoothed targets:\n", smoothed_targets)
print("Loss:", loss.item())
```

å°±æ˜¯æŠŠæœ¬æ¥æ­£ç¡®çš„`one-hot`ç¼–ç ï¼Œä»`[0, 1, 0, 0, ..., 0]`å˜æˆ `[0.00101, 0.901, 0.00101, ..., 0.00101]`

ç„¶åå»è®¡ç®—loss

## é™„

### å±‚å½’ä¸€åŒ–ï¼ˆLayerNormï¼‰

ä¸¾ä¸ªä¾‹å­ï¼š
ç­ä¸Šæœ‰ 5 ä¸ªåŒå­¦çš„èº«é«˜ï¼š
`[140, 150, 160, 170, 180]` cm
è€å¸ˆå…ˆç®—å‡ºå¹³å‡èº«é«˜ `160`ï¼Œç„¶åå‡å»å¹³å‡ã€é™¤ä»¥â€œæ³¢åŠ¨å¤§å°â€ï¼ˆæ ‡å‡†å·®ï¼‰ï¼Œå¾—åˆ°ä¸€æ’æ¥è¿‘ 0 çš„æ•°`[-1.6, â€‘0.8, 0, 0.8, 1.6]`
æœ€åå†ä¹˜ä¸€ä¸ªâ€œæ”¾å¤§å€æ•°â€ Î³ï¼ˆgammaï¼‰å’ŒåŠ ä¸€ä¸ªâ€œå¹³ç§»â€ Î²ï¼ˆbetaï¼‰ï¼Œè®©ç½‘ç»œè‡ªå·±å†³å®šåˆ°åº•è¦å¤šå¤§ã€‚

```python
import torch

class MyLayerNorm(torch.nn.Module):
    def __init__(self, dim):
        super().__init__()
        # ä¸¤ä¸ªå¯è®­ç»ƒçš„å‚æ•°ï¼šç¼©æ”¾ gammaã€å¹³ç§» beta
        self.gamma = torch.nn.Parameter(torch.ones(dim))
        self.beta  = torch.nn.Parameter(torch.zeros(dim))

    def forward(self, x):
        # x å½¢çŠ¶ï¼š(batch, seq_len, dim)
        mean = x.mean(dim=-1, keepdim=True)        # 1) æ±‚å¹³å‡
        var  = x.var (dim=-1, keepdim=True)        # 2) æ±‚æ–¹å·®
        x_hat = (x - mean) / torch.sqrt(var + 1e-5)  # 3) æ ‡å‡†åŒ–
        return self.gamma * x_hat + self.beta      # 4) ç¼©æ”¾+å¹³ç§»
```

### Dropout

å­¦ä¹ æ—¶éšæœºâ€œä¸¢â€æ‰ä¸€äº›æ•°å­—ï¼Œé˜²æ­¢æ­»è®°ç¡¬èƒŒã€‚

```python
class MyDropout(torch.nn.Module):
    def __init__(self, p=0.1):
        super().__init__()
        self.p = p          # ä¸¢å¼ƒæ¦‚ç‡

    def forward(self, x):
        if not self.training:        # å¦‚æœæ˜¯æ¨ç†æ¨¡å¼ï¼Œç›´æ¥è¿”å›åŸå€¼
            return x
        mask = (torch.rand_like(x) > self.p).float()  # éšæœº 0/1 æ©ç 
        return x * mask / (1 - self.p)                # æ”¾å¤§ä¿ç•™çš„å€¼
```

### æ®‹å·®è¿æ¥

AIå›ç­”å¦‚ä¸‹ï¼š

> å†™ä½œæ–‡ï¼š
>
> 1. å…ˆå†™äº†ä¸€æ®µè‰ç¨¿ï¼ˆè¿™å°±æ˜¯**è¾“å…¥ x**ï¼‰ã€‚
> 2. è€å¸ˆå¸®ä½ ä¿®æ”¹äº†ä¸€éï¼Œå¾—åˆ°â€œä¿®æ”¹ç‰ˆâ€ï¼ˆè¿™å°±æ˜¯**å¤„ç†åçš„ç»“æœ F(x)**ï¼‰ã€‚
> 3. ä½†è€å¸ˆæ€•ä½ æ”¹å¾—é¢ç›®å…¨éï¼Œäºæ˜¯è®©ä½ æŠŠ**åŸè‰ç¨¿ x**å†å åœ¨æœ€ä¸‹é¢ï¼Œä¸€èµ·äº¤ä¸Šå»ã€‚
>
> æœ€ç»ˆä½œæ–‡ = åŸè‰ç¨¿ + è€å¸ˆä¿®æ”¹
> è¿™å°±å«â€œæ®‹å·®è¿æ¥â€ã€‚

```python
import torch
import torch.nn as nn

class SimpleBlock(nn.Module):
    def __init__(self):
        super().__init__()
        self.layer = nn.Linear(64, 64)   # éšä¾¿ä¸€ä¸ªå¤„ç†å±‚

    def forward(self, x):
        out = self.layer(x)   # å¤„ç†
        return x + out        # æ®‹å·®è¿æ¥ï¼šæŠŠ x ç›´æ¥åŠ å›æ¥
```

