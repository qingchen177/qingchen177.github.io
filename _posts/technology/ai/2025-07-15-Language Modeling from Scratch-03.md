---
title: "Stanford CS336 | Language Modeling from Scratch | 03"
subtitle: "EVERYTHING YOU DIDN'T WANT TO KNOW ABOUT LM ARCHITECTURE AND TRAINING"
layout: post
tag: ["LLM", "Stanford" ,"cs336"]
categories: ["äººå·¥æ™ºèƒ½"]
---

## å‰è¨€

è¯¾ç¨‹é“¾æ¥ï¼š[Language Modeling from Scratch](https://online.stanford.edu/courses/cs336-language-modeling-scratch)

> `recap`ï¼šæ‰¼è¦é‡è¿°ï¼›æ¦‚æ‹¬ï¼›ç®€è¦å›é¡¾ï¼›é‡è¿°è¦ç‚¹ï¼›<æ–°é—»>ç®€æ˜æ–°é—»ï¼›èƒé¢ç¿»æ–°çš„è½®èƒ
>
> `variations`ï¼šå˜åŒ–ï¼Œå˜æ›´ï¼Œå˜å¼‚ï¼›å˜ä½“ï¼›å˜ç§ï¼›å˜å¥æ›²ï¼›å˜å¥ï¼›å˜å¼‚çš„ä¸œè¥¿
>
> `empirically`ï¼šå‡­ç»éªŒ
>
> `evidence`ï¼šè¯æ®
>
> `monolingual`ï¼šå•è¯­çš„
>
> `multilingual`ï¼šå¤šè¯­è¨€çš„
>
> `rule of thumb`ï¼šç»éªŒæ³•åˆ™ï¼Œç²—ç•¥ä¼°ç®—;ç»éªŒä¹‹è°ˆ



themeï¼š
- the best way to learn is hands-on experience
- the second best way is to try to learn from othersâ€™ experience

## Start

æ ‡å‡†çš„transformerçš„é€‰æ‹©ï¼š

- ä½ç½®ç¼–ç 
- æ¿€æ´»å‡½æ•°ï¼š`ReLU`
- å½’ä¸€åŒ–æ–¹å¼ï¼š`post-normï¼ˆåå½’ä¸€åŒ–ï¼‰ï¼ŒLayerNormï¼ˆå±‚å½’ä¸€åŒ–ï¼‰`

![image-20250725140017941](/assets/images/post/image-20250725140017941.png)

æˆ‘ä»¬è¦å®ç°çš„æ˜¯ï¼šç®€å•ã€ç°ä»£å˜ä½“

- å½’ä¸€åŒ–æ–¹å¼ï¼š`pre-normï¼ˆå‰å½’ä¸€åŒ–ï¼‰`
- æ—‹è½¬ä½ç½®ç¼–ç ï¼š`RoPEï¼ˆRotary position embeddingsï¼‰`
- æ¿€æ´»å‡½æ•°ï¼š`SwiGLU`
- çº¿æ€§å±‚å’Œå±‚å½’ä¸€åŒ–æ²¡æœ‰åç§»é¡¹

![image-20250725140550503](/assets/images/post/image-20250725140550503.png)

ç°åœ¨çš„æ¨¡å‹æ¶æ„ï¼š

![image-20250725141138541](/assets/images/post/image-20250725141138541.png)

> ä¸Šé¢è¿™å¼ PDFé‡Œé¢æœ¬èº«å°±å¥½æ¨¡ç³Š

![image-20250725141540425](/assets/images/post/image-20250725141540425.png)

High level view:

- Low consensus(except pre-norm)`ä½å…±è¯†ï¼Œé™¤äº†å‰å½’ä¸€åŒ–`
- Trends toward â€˜LLaMA-likeâ€™ architectures `éƒ½æ˜¯LLaMAçš„æ¶æ„è¶‹åŠ¿`

## å½’ä¸€åŒ–

### å‰VSå

- åï¼š`x â†’ sublayer â†’ +x â†’ LayerNorm â†’ next sublayer`
- å‰ï¼š`x â†’ LayerNorm â†’ sublayer â†’ +x â†’ next sublayer`

![image-20250725141949541](/assets/images/post/image-20250725141949541.png)

> Set up LayerNorm so that it doesnâ€™t affect the main residual signal path (on the left)
>
> è®¾ç½®LayerNormï¼Œä½¿å…¶ä¸å½±å“ä¸»å‰©ä½™ä¿¡å·è·¯å¾„ï¼ˆåœ¨å·¦ä¾§ï¼‰

![image-20250725142104028](/assets/images/post/image-20250725142104028.png)

> - `Original stated advantage â€“ removing warmup`
>   æ—©æœŸå¤§å®¶æŠŠ LayerNorm æ”¾åœ¨æ®‹å·®æ”¯è·¯çš„â€œåé¢â€ï¼ˆPost-LNï¼Œåå½’ä¸€åŒ–ï¼‰ï¼Œç†ç”±æ˜¯ï¼šåªè¦æŠŠè¾“å‡ºç›´æ¥å½’ä¸€åŒ–ï¼Œå°±èƒ½æŠŠæ–¹å·®å‹åˆ° 1ï¼Œç†è®ºä¸Šå°±ä¸å†éœ€è¦â€œå­¦ä¹ ç‡ warmupâ€ã€‚
> - `Today â€“ stability and larger LRs for large networks`
>   å¯åˆ°äº†ä»Šå¤©ï¼Œäººä»¬æ›´å–œæ¬¢æŠŠ LayerNorm æ”¾åœ¨æ®‹å·®æ”¯è·¯çš„â€œå‰é¢â€ï¼ˆPre-LNï¼Œå‰å½’ä¸€åŒ–ï¼‰ã€‚å› ä¸ºçœŸæ­£è®­è¶…å¤§æ¨¡å‹æ—¶ï¼ŒPost-LN ä¾ç„¶å®¹æ˜“æ¢¯åº¦çˆ†ç‚¸/æ¶ˆå¤±ï¼ŒPre-LN åè€Œæ›´ç¨³ï¼Œè¿˜èƒ½ç”¨æ›´å¤§çš„å­¦ä¹ ç‡ã€‚

| ç»´åº¦       | Post-LN             | Pre-LN                           |
| ---------- | ------------------- | -------------------------------- |
| æ·±å±‚æ¢¯åº¦   | å®¹æ˜“`vanishing`æ¶ˆå¤± | è¿‘ä¼¼æ’ç­‰æ˜ å°„ï¼Œæ¢¯åº¦æ›´ç¨³           |
| å­¦ä¹ ç‡     | éœ€è¦å° LR + warmup  | å¯ä»¥ç›´æ¥ä¸Šè¾ƒå¤§ LR                |
| å¤§æ¨¡å‹è®­ç»ƒ | éœ€è¦å¤§é‡è°ƒå‚é˜²å´©    | å‡ ä¹â€œå¼€ç®±å³ç”¨â€                   |
| æœ€ç»ˆæ€§èƒ½   | ç¨å¥½ï¼ˆæ—©æœŸå°æ¨¡å‹ï¼‰  | å·®å¼‚æå°ï¼Œè®­ç»ƒæ•ˆç‡è¿œé«˜äºæ€§èƒ½å·®å¼‚ |

### åŒå±‚å½’ä¸€åŒ–

![image-20250725142723389](/assets/images/post/image-20250725142723389.png)

> Recent models: Grok, Gemma 2. Olmo 2 only does non-residual post norm

### LayerNorm vs RMSNorm

**å±‚å½’ä¸€åŒ–**

å¹³å‡å€¼å’Œæ–¹å·®
$$
y=\frac{x-E[x]}{\sqrt{Var[x]+\epsilon}}*\gamma+\beta
$$
$\epsilon$ï¼šè¯¶å™—è‰²éš†æ˜¯å°å¸¸æ•°ï¼ˆé˜²æ­¢é™¤0ï¼‰

$\gamma$ï¼šä¼½é©¬æ˜¯å¯å­¦ä¹ å‚æ•°

$\beta$ï¼šè´å¡”ä¹Ÿæ˜¯å¯å­¦ä¹ å‚æ•°

**RMSNorm**

LNçš„å‡å€¼ Î¼ ä¸º0æ—¶ä¾¿æ˜¯RMSNorm

ï¼ˆRoot Mean Square Normalizationï¼‰
$$
RMSNorm(x)=\frac{x}{RMS(x)}Â·\gammaï¼Œå…¶ä¸­RMS(x)=\sqrt{\frac{1}{n}\sum_{i=1}^{n}{x_i^2} }
$$
RMSNormè®¡ç®—æ›´å°‘ï¼Œå­˜çš„å‚æ•°ä¹Ÿæ›´å°‘ï¼Œå› ä¸ºæ²¡æœ‰`bias`
$$
y= \frac{x}{\sqrt{\left | \left | x \right |  \right |_2^2+\epsilon }}*\gamma
$$
ä»£ç ç¤ºä¾‹ï¼š

```python
import numpy as np

x = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=float)

# 1. LayerNorm (æ— å­¦ä¹ å‚æ•° Î³, Î² æ—¶)
mu = x.mean()
var = x.var(ddof=0)
layer_norm = (x - mu) / np.sqrt(var + 1e-8)   # 1e-8 é˜²æ­¢é™¤é›¶

# 2. RMSNorm (æ— å­¦ä¹ å‚æ•° Î³ æ—¶)
rms = np.sqrt(np.mean(x ** 2))
rms_norm = x / (rms + 1e-8)

print("åŸå§‹æ•°æ®:   ", x)
print("LayerNorm: ", layer_norm)
print("RMSNorm:   ", rms_norm)

# åŸå§‹æ•°æ®:    [1. 2. 3. 4. 5. 6. 7. 8. 9.]
# LayerNorm:  [-1.54919334 -1.161895   -0.77459667 -0.38729833 0.23434234  0.38729833 0.77459667  1.161895    1.54919334]
# RMSNorm:    [0.17770466 0.35540933 0.53311399 0.71081865 0.88852332 1.06622798 1.24393264 1.4216373  1.59934197]
```

| **ç‰¹æ€§**       | **LayerNorm**                         | **RMSNorm**                       |
| -------------- | ------------------------------------- | --------------------------------- |
| **å‡å€¼ä¸­å¿ƒåŒ–** | å‡å»å‡å€¼ $\mu = \frac{1}{n}\sum x_i$  | çœç•¥ï¼ˆå‡è®¾å‡å€¼ä¸º0ï¼‰               |
| **æ–¹å·®è®¡ç®—**   | åŸºäºä¸­å¿ƒåŒ–åçš„å€¼ $\sum (x_i - \mu)^2$ | ç›´æ¥åŸºäºåŸå§‹å€¼ $\sum x_i^2$       |
| **å‚æ•°**       | å¢ç›Š $\gamma$ å’Œåç½® $\beta$          | ä»…å¢ç›Š $\gamma$ï¼ˆé€šå¸¸æ— åç½®ï¼‰     |
| **è®¡ç®—æ•ˆç‡**   | è¾ƒä½ï¼ˆéœ€è®¡ç®—å‡å€¼å’Œæ–¹å·®ï¼‰              | è¾ƒé«˜ï¼ˆå‡å°‘çº¦7%è®­ç»ƒæ—¶é—´ï¼‰          |
| **æ€§èƒ½**       | åŸºå‡†                                  | åœ¨å¤šæ•°ä»»åŠ¡ä¸­ä¸LayerNormç›¸å½“æˆ–æ›´ä¼˜ |

## ä¸¢å¼ƒåç½®é¡¹

`dropping bias terms`
$$
FFN(x)=\sigma(xW_1)W_2
$$

> Reasons: memory (similar to RMSnorm) and optimization stability
>
> å‡å°‘å†…å­˜å’Œä¼˜åŒ–å™¨ç¨³å®š

## æ¿€æ´»å‡½æ•°

æ¿€æ´»å‡½æ•°å…¨å®¶æ¡¶ï¼š

`ReLU, GeLU, Swish, ELU, GLU, GeGLU, ReGLU, SeLU, SwiGLU, LiGLU`

### ReLU

$$
FF(x)=max(0,xW_1)W_2
$$

### GeLU

$$
FF(x)=GELU(xW_1)W_2
$$

$$
GELU(x)=xÂ·\Phi(x)=xÂ·\frac{1}{2}[1+erf(\frac{x}{\sqrt{2}})]
$$

- Î¦(*x*) æ˜¯é«˜æ–¯åˆ†å¸ƒçš„ç´¯ç§¯åˆ†å¸ƒå‡½æ•°ï¼ˆCDFï¼‰ã€‚
- erf æ˜¯è¯¯å·®å‡½æ•°ï¼ˆerror functionï¼‰ã€‚

**å…³é”®ç‰¹æ€§**

- **å¹³æ»‘æ€§**ï¼šå¤„å¤„å¯å¯¼ï¼Œé¿å…äº†ReLUçš„â€œç¡¬æˆªæ–­â€ã€‚
- **éå•è°ƒæ€§**ï¼šè´ŸåŠè½´æœ‰å¾®å°è¾“å‡ºï¼ˆä¸ReLUä¸åŒï¼‰ã€‚
- **æ¦‚ç‡è§£é‡Š**ï¼šå¯è§†ä¸ºå¯¹è¾“å…¥éšæœºé™æƒçš„æœŸæœ›ï¼ˆä»¥æ¦‚ç‡Î¦(*x*)ä¿ç•™xï¼‰ã€‚

![image-20250725153321116](/assets/images/post/image-20250725153321116.png)

### GLU

**Gated Linear Unit**çš„æ€æƒ³æ˜¯ï¼š**ä¸æ˜¯ç®€å•åœ°ç”¨ ReLU æ¿€æ´»æ•´ä¸ªçº¿æ€§è¾“å‡ºï¼Œè€Œæ˜¯å¼•å…¥ä¸€ä¸ªâ€œé—¨æ§â€æœºåˆ¶ï¼Œæ§åˆ¶å“ªäº›ä¿¡æ¯å¯ä»¥é€šè¿‡ã€‚**

åŸå§‹ GLU çš„å½¢å¼ä¸€èˆ¬æ˜¯ï¼š
$$
GLU(x)=(xW_1)âŠ—Ïƒ(xV)â‹…W_2
$$


å…¶ä¸­ âŠ— æ˜¯é€å…ƒç´ ä¹˜æ³•ï¼ˆHadamard ç§¯ï¼‰ï¼Œ*Ïƒ* æ˜¯ sigmoid å‡½æ•°ï¼Œ*V* æ˜¯å¦ä¸€ä¸ªå¯å­¦ä¹ çš„å‚æ•°çŸ©é˜µã€‚é—¨æ§ä¿¡å·ç”±$Ïƒ(xV)$  äº§ç”Ÿï¼Œç”¨æ¥è°ƒåˆ¶$ xW_1 $â€‹â€‹çš„è¾“å‡ºã€‚

> å“ˆè¾¾ç›ç§¯(Hadamard product)ï¼šè‹¥A=(aij)å’ŒB=(bij)æ˜¯ä¸¤ä¸ªåŒé˜¶çŸ©é˜µï¼Œè‹¥cij=aijÃ—bij,åˆ™ç§°çŸ©é˜µC=(cij)ä¸ºAå’ŒBçš„å“ˆè¾¾ç›ç§¯ï¼Œæˆ–ç§°åŸºæœ¬ç§¯

$$
\begin{bmatrix}
a_{11}b_{11}& a_{12}b_{12}&   ...& a_{1n}b_{1n}& \\\\\\
a_{21}b_{21}& a_{22}b_{22}&   ...& a_{2n}b_{2n}& \\\\\\
\vdots&  \vdots&  \vdots&  \vdots& \\\\\\
a_{m1}b_{m1}& a_{m2}b_{m2}&   ...& a_{mn}b_{mn}&
\end{bmatrix}
$$



### ReGLU

ä½¿ç”¨ **ReLU è€Œä¸æ˜¯ sigmoid** æ¥åšé—¨æ§
$$
FF_{ReGLU}(x)=(max(0,xW_1)âŠ—(xV))W_2
$$

### GeGLU

$$
FFN_{GEGLU}(x,W,V,W_2)=(GELU(xW)\otimes xV)W_2
$$

### SwiGLU

swish is x * sigmoid(x)
$$
FFN_{SwiGLU}(x,W,V,W_2)=(Swish_1(xW)\otimes xV)W_2
$$

> Note: Gated models use smaller dimensions for the ğ‘‘ğ‘“ğ‘“ by 2/3
>
> åœ¨ä½¿ç”¨é—¨æ§æœºåˆ¶å¦‚ GLUã€ReGLUã€SwiGLU ç­‰ï¼‰çš„å‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰ä¸­ï¼Œä¸ºäº†ä¿æŒä¸æ ‡å‡† FFN ç›¸å½“çš„å‚æ•°é‡æˆ–è®¡ç®—é‡ï¼Œé€šå¸¸ä¼šå°†ä¸­é—´å±‚çš„ç»´åº¦$d_{ff}$â€‹ï¼ˆå³éšè—å±‚å®½åº¦ï¼‰**è®¾ç½®ä¸ºåŸæ¥å¤§å°çš„çº¦ 2/3**ã€‚
>
> 
>
> å› ä¸ºå¤šäº†ä¸ª$V$ï¼Œå‡è®¾ $d_{model}=1024$ ï¼Œæ ‡å‡† FFN ä¸­ $d_{ff}=4096$ï¼š
>
> - æ ‡å‡† FFN å‚æ•°ï¼ˆå‰ä¸¤çŸ©é˜µï¼‰ï¼š
>
>   $1024Ã—4096+4096Ã—1024â‰ˆ8.4M$
>
> - ReGLU å¦‚æœä¹Ÿç”¨ $d_{ff}=4096$ï¼š
>
>   - $W_1:1024Ã—4096$
>   - $V:1024Ã—4096$
>   - $W_2:4096Ã—1024$
>   - æ€»å‚æ•°ï¼š3Ã—(1024Ã—4096)â‰ˆ12.6*M* â†’ å¤šäº† 50%ï¼
>
> - æ‰€ä»¥æŠŠ$d_{ff} æ”¹ä¸º \frac{2}{3}Ã—4096â‰ˆ2730$
>
>   - å‚æ•°å˜ä¸ºï¼š$2Ã—(1024Ã—2730)+2730Ã—1024â‰ˆ8.4M$

## ä¸²è¡Œå’Œå¹¶è¡Œå±‚

![image-20250729111836895](/assets/images/post/image-20250729111836895.png)

å¦‚æœå®ç°æ­£ç¡®ï¼ŒLayerNormå¯ä»¥å…±äº«ï¼ŒçŸ©é˜µä¹˜æ³•å¯ä»¥èåˆ

## æ¶æ„æ€»ç»“

![image-20250729111758437](/assets/images/post/image-20250729111758437.png)

## ä½ç½®ç¼–ç 

### RoPE

**rotary position embeddings**

çœ‹ä¸‹è‹ç¥çš„æ–‡ç« ï¼š

- https://kexue.fm/archives/8130
- https://kexue.fm/archives/8231
- https://kexue.fm/archives/8265
- https://kexue.fm/archives/8397
- https://papers.cool/arxiv/2104.09864

![image-20250729134318233](/assets/images/post/image-20250729134318233.png)

![image-20250729134343092](/assets/images/post/image-20250729134343092.png)

![image-20250729134353417](/assets/images/post/image-20250729134353417.png)

> okï¼ŒçŸ­æ—¶é—´ç†è§£ä¸äº†ï¼Œåé¢å†è¡¥ï¼ï¼ï¼

## è¶…å‚æ•°

è®²äº†ä¸€äº›å…±è¯†è¶…å‚æ•°`consensus hyperparameters`

### å‰é¦ˆå±‚-æ¨¡å‹ç»´åº¦æ¯”ä¾‹ 

`Feedforward â€“ model dimension ratio`
$$
d_{ff}=4d_{model}
$$
ç”¨äº†GLUçš„è¯ï¼Œéœ€è¦æ³¨æ„ï¼š

![image-20250729135254020](/assets/images/post/image-20250729135254020.png)

è¿˜æœ‰ä¸€ä¸ªå¤§èƒ†çš„`T5`ç”¨äº†`64`å€

![image-20250729135444211](/assets/images/post/image-20250729135444211.png)

æ ¹æ®ç»éªŒï¼Œè¿™ä¸ªè¶…å‚æ•°åœ¨1-10ä¹‹é—´æœ‰ä¸€ä¸ªç›†åœ°ä½¿lossæœ€ä¼˜

![image-20250729135621779](/assets/images/post/image-20250729135621779.png)

### å¤šå¤´æ³¨æ„åŠ›æ•°é‡-æ¨¡å‹ç»´åº¦æ¯”ä¾‹

`Head-dim * num-heads to model-dim ratio`

æˆ‘ä»¬å¯ä»¥æœ‰`head-dimensions > model-dim / num-heads`ï¼Œä½†å¤§å¤šæ•°æ¨¡å‹ç¡®å®éµå¾ªè¿™ä¸€æŒ‡å¯¼æ–¹é’ˆã€‚

### çºµæ¨ªæ¯” - Aspect ratios

![image-20250729140637771](/assets/images/post/image-20250729140637771.png)

ææ·±çš„æ¨¡å‹æ›´éš¾å¹¶è¡ŒåŒ–ï¼Œå»¶è¿Ÿä¹Ÿæ›´é«˜

### è¯è¡¨å¤§å° - vocabulary sizes

![image-20250729141140719](/assets/images/post/image-20250729141140719.png)

### Dropout and other regularization

dropoutå’Œæ­£åˆ™åŒ–

![image-20250729141552104](/assets/images/post/image-20250729141552104.png)

### æ€»ç»“

- feedforwardï¼šç»éªŒéƒ½æ˜¯4ä¸ºæ ‡å‡†
- head dimï¼š$d_{head} * N_{haed} = d_{model}$æ˜¯æ ‡é…ï¼Œä¸è¿‡ä½ä¸€ç‚¹ä¹Ÿæ²¡éªŒè¯è¡Œä¸è¡Œ
- aspect ratioï¼šè‰¯å¥½çš„å€¼èŒƒå›´åŒºé—´åœ¨`100-200`ï¼Œå¤ªæ·±ç¡¬ä»¶ä¹Ÿè·Ÿä¸ä¸Š
- regularizationï¼šè¿˜æ˜¯è¦æ­£åˆ™åŒ–ï¼Œ`but its effects are primarily on optimization dynamics`

## ç¨³å®šæ€§æ‰‹æ®µ-Stability tricks

### Softmaxes 

`Softmaxes  â€“ can be ill-behaved due to exponentials / divison by zero`

`Softmax`å¯èƒ½ç”±äºæŒ‡æ•°/é™¤ä»¥é›¶è€Œè¡¨ç°ä¸ä½³

![image-20250729142618960](/assets/images/post/image-20250729142618960.png)

### Attention softmax stability â€“ the â€˜QK normâ€™

> The query and keys are Layer (RMS) normed before going into the softmax operation.

åœ¨softmaxä¹‹é—´å¯¹QKè¿›è¡Œæ­£åˆ™åŒ–ã€‚

### Logit soft-capping

> **Logit Soft-Capping** æ˜¯ä¸€ç§åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰**æ¨ç†é˜¶æ®µ**ï¼ˆæˆ–è®­ç»ƒä¸­ï¼‰ç”¨äº**æ§åˆ¶è¾“å‡º logits èŒƒå›´**çš„æŠ€æœ¯ï¼Œç›®çš„æ˜¯**æŠ‘åˆ¶æ¨¡å‹ç”Ÿæˆè¿‡äºæç«¯æˆ–é‡å¤çš„æ–‡æœ¬**ï¼Œæå‡ç”Ÿæˆè´¨é‡ã€‚
>
> å®ƒè¢«ç”¨äºä¸€äº›å…ˆè¿›æ¨¡å‹ä¸­ï¼Œæ¯”å¦‚ **Google çš„ Gemmaã€PaLMã€LLaMA-3 çš„æ¨ç†è¿‡ç¨‹**ä¸­ï¼Œä½œä¸ºåå¤„ç† logits çš„ä¸€ç§â€œè½¯é™åˆ¶â€æ‰‹æ®µã€‚

![image-20250729142927898](/assets/images/post/image-20250729142927898.png)

ç»™å®šä¸€ä¸ªåŸå§‹çš„ logit å€¼ *z* ï¼Œsoft-capping é€šè¿‡å¦‚ä¸‹å‡½æ•°è¿›è¡Œå˜æ¢ï¼š
$$
SoftCap(z)=câ‹…tanh(\frac{z}{c})
$$
å…¶ä¸­ï¼š

- *z* ï¼šæ¨¡å‹è¾“å‡ºçš„åŸå§‹ logitï¼ˆæŸä¸ªè¯çš„å¾—åˆ†ï¼‰
- *c* ï¼š**capping å€¼**ï¼ˆä¾‹å¦‚ 30 æˆ– 50ï¼‰ï¼Œè¡¨ç¤º logits çš„â€œè½¯ä¸Šé™â€
- tanh ï¼šåŒæ›²æ­£åˆ‡å‡½æ•°ï¼ŒæŠŠè¾“å…¥å‹ç¼©åˆ° (âˆ’1,1) åŒºé—´ï¼Œä¹˜ä»¥ *c* åå‹ç¼©åˆ° (âˆ’*c*,*c*)

æ‰€ä»¥æœ€ç»ˆè¾“å‡ºçš„ logit è¢«â€œè½¯æ€§åœ°â€é™åˆ¶åœ¨ [âˆ’*c*,*c*] èŒƒå›´å†…ã€‚

å‡½æ•°ï¼š$f(z)=câ‹…tanh(z/c)$

| è¾“å…¥*z*        | è¾“å‡º*f*(*z*) | è¡Œä¸º       |
| -------------- | ------------ | ---------- |
| å¾ˆå°ï¼ˆè´Ÿå¾ˆå¤§ï¼‰ | â‰ˆ -c         | è¶‹è¿‘ä¸‹ç•Œ   |
| 0              | 0            | ä¸å˜       |
| å¾ˆå¤§ï¼ˆæ­£å¾ˆå¤§ï¼‰ | â‰ˆ c          | è¶‹è¿‘ä¸Šç•Œ   |
| ä¸­ç­‰å¤§å°       | â‰ˆ z          | å‡ ä¹æ— å½±å“ |

ç‰¹ç‚¹ï¼š

- å¯¹**ä¸­ç­‰å¤§å°çš„ logits** å‡ ä¹ä¸æ”¹å˜
- å¯¹**æå¤§æˆ–æå°çš„ logits** è¿›è¡Œâ€œæ¸©å’Œå‹åˆ¶â€ï¼Œä¸è®©å®ƒä»¬ä¸»å¯¼ softmax
- æ˜¯**å¹³æ»‘ã€å¯å¯¼çš„å‡½æ•°**ï¼Œä¸ä¼šç ´åæ¢¯åº¦ï¼ˆå¯ç”¨äºè®­ç»ƒï¼‰

åœ¨ç”Ÿæˆæ–‡æœ¬æ—¶ï¼ŒæŸäº› token çš„ logit å¯èƒ½éå¸¸å¤§ï¼ˆæ¯”å¦‚æŸä¸ªè¯è¢«å¼ºçƒˆåå¥½ï¼‰ï¼Œå¯¼è‡´ï¼š

- softmax åæ¦‚ç‡æ¥è¿‘ 1
- å…¶ä»–è¯å‡ ä¹æ²¡æœºä¼šè¢«é‡‡æ ·
- å®¹æ˜“å¼•å‘ï¼š
  - **é‡å¤ç”Ÿæˆ**ï¼ˆå¦‚â€œthe the theâ€ï¼‰
  - **å¤šæ ·æ€§å·®**
  - **å¹»è§‰å¢å¼º**ï¼ˆè¿‡åº¦è‡ªä¿¡é”™è¯¯å†…å®¹ï¼‰

**Soft-Capping**

ç”¨ *c*=30 ä¸¾ä¾‹ï¼š

$SoftCap(100)=30â‹…tanh(100/30)â‰ˆ30â‹…tanh(3.33)â‰ˆ30â‹…0.997â‰ˆ29.9$

è€ŒåŸæœ¬æ˜¯ 100 â†’ ç°åœ¨è¢«å‹åˆ°æ¥è¿‘ 30

è¿™æ ·åœ¨ softmax ä¸­ï¼Œå®ƒä»ç„¶å¾ˆé«˜ï¼Œä½†**ä¸å†å‹å€’æ€§åœ°ä¸»å¯¼**ï¼Œå…¶ä»–åˆç†è¯ä¹Ÿæœ‰æœºä¼šè¢«é‡‡æ ·ã€‚

### Attention heads

æ ‡å‡†çš„å¤šå¤´æ³¨æ„åŠ›ä¸­ï¼Œæ¯ä¸ªå¤´æœ‰ç‹¬ç«‹çš„QKV

1. å¯¹äºç¬¬iä¸ªå¤´ï¼Œè®¡ç®—ï¼š
   $$
   Q_i=XW_i^Q,\ K_i=XW_i^K,\ V_i=XW_i^V
   $$

2. ç„¶ååˆ†åˆ«è®¡ç®—æ³¨æ„åŠ›ï¼š
   $$
   Attention(Q_i,K_i,V_i)
   $$

3. æœ€åå°†æ‰€æœ‰å¤´çš„è¾“å‡ºæ‹¼æ¥å¹¶çº¿æ€§å˜æ¢$W_O$â€‹

ç°å®ä¸ºäº†å‡å°‘å®é™…çš„æ³¨æ„åŠ›æ¶ˆè€—ï¼š

Reducing attention head cost GQA/MQA

#### **MQAï¼ˆMulti-Query Attentionï¼‰**

æ‰€æœ‰æ³¨æ„åŠ›å¤´å…±äº«åŒä¸€ä¸ª K å’Œ Vï¼Œä½†æ¯ä¸ªå¤´ä»ç„¶æœ‰è‡ªå·±çš„ Qã€‚
$$
Q_i=XW_i^Q,\ K=XW^K,\ V=XW^V
$$
K,Vå…±ç”¨åªæœ‰Qä¸ä¸€æ ·ã€‚

> ç¼ºç‚¹ï¼š
>
> è¡¨è¾¾èƒ½åŠ›ä¸‹é™ï¼šå› ä¸ºæ‰€æœ‰å¤´å…±äº«ç›¸åŒçš„ Key å’Œ Valueï¼Œé™åˆ¶äº†æ¨¡å‹çš„è¡¨è¾¾çµæ´»æ€§ï¼Œå¯èƒ½å½±å“æ¨¡å‹è´¨é‡ï¼ˆå°¤å…¶åœ¨å¤æ‚ä»»åŠ¡ä¸Šï¼‰ã€‚

![image-20250729144712709](/assets/images/post/image-20250729144712709.png)

#### GQAï¼ˆGrouped-Query Attentionï¼‰

#### æ ¸å¿ƒæ€æƒ³ï¼š

- **æŠ˜ä¸­æ–¹æ¡ˆ**ï¼šä»‹äº MHA å’Œ MQA ä¹‹é—´ã€‚
- å°†å¤šä¸ªå¤´ **åˆ†ç»„**ï¼Œæ¯ç»„å…±äº«ä¸€ç»„ K å’Œ Vã€‚
- ä¾‹å¦‚ï¼šå¦‚æœæœ‰ 32 ä¸ªå¤´ï¼Œåˆ†æˆ 4 ç»„ï¼Œæ¯ç»„ 8 ä¸ªå¤´ï¼Œé‚£ä¹ˆå°±æœ‰ 4 ä¸ªä¸åŒçš„ K å’Œ Vã€‚

![image-20250729144756812](/assets/images/post/image-20250729144756812.png)

### å¯¹æ¯”æ€»ç»“

| æ–¹æ³•        | Query    | Key      | Value    | KV Cache å¤§å° | è¡¨è¾¾èƒ½åŠ› | æ¨ç†æ•ˆç‡ |
| ----------- | -------- | -------- | -------- | ------------- | -------- | -------- |
| MHAï¼ˆæ ‡å‡†ï¼‰ | æ¯å¤´ç‹¬ç«‹ | æ¯å¤´ç‹¬ç«‹ | æ¯å¤´ç‹¬ç«‹ | é«˜ï¼ˆH å€ï¼‰    | æœ€å¼º     | æœ€ä½     |
| GQA         | æ¯å¤´ç‹¬ç«‹ | æ¯ç»„å…±äº« | æ¯ç»„å…±äº« | ä¸­ç­‰ï¼ˆG å€ï¼‰  | è¾ƒå¼º     | é«˜       |
| MQA         | æ¯å¤´ç‹¬ç«‹ | å…¨å±€å…±äº« | å…¨å±€å…±äº« | æœ€ä½ï¼ˆ1 å€ï¼‰  | è¾ƒå¼±     | æœ€é«˜     |

> Hï¼šæ€»å¤´æ•°ï¼ŒGï¼šç»„æ•°ï¼ˆ1 â‰¤ G â‰¤ Hï¼‰ 

#### Sparse / sliding window attention

**ç¨€ç–æ³¨æ„åŠ›ï¼ˆSparse Attentionï¼‰**

****

![image-20250729145142611](/assets/images/post/image-20250729145142611.png)

**SWAæ»‘åŠ¨çª—å£æ³¨æ„åŠ›ï¼ˆSliding Window Attentionï¼‰**

 å…·ä½“æ¥è¯´ï¼š

- å¯¹äºä½ç½® *i* çš„ tokenï¼Œå®ƒåªä¸å‰å *w* ä¸ª token è®¡ç®—æ³¨æ„åŠ›ï¼ˆå³ä¸€ä¸ªå¤§å°ä¸º 2*w*+1 çš„â€œæ»‘åŠ¨çª—å£â€ï¼‰ã€‚
- çª—å£éšä½ç½®ç§»åŠ¨ï¼Œåƒå·ç§¯ä¸€æ ·æ»‘è¿‡åºåˆ—ã€‚

ç¤ºä¾‹ï¼š

- å¦‚æœçª—å£å¤§å°æ˜¯ 512ï¼Œåˆ™æ¯ä¸ª token æœ€å¤šä¸å‰å 256 ä¸ª token æ³¨æ„ã€‚
- å¤æ‚åº¦ä» *O*(*n*2) é™åˆ° *O*(*n*Ã—*w*) ï¼Œå½“ *w*â‰ª*n* æ—¶å¤§å¹…é™ä½ã€‚

ç‰¹ç‚¹ï¼š

- å±€éƒ¨å»ºæ¨¡èƒ½åŠ›å¼ºï¼ˆé€‚åˆå±€éƒ¨ä¾èµ–ï¼Œå¦‚è¯­æ³•ã€å±€éƒ¨ä¸Šä¸‹æ–‡ï¼‰ã€‚
- æ— æ³•å»ºæ¨¡è¿œè·ç¦»ä¾èµ–ï¼ˆæ¯”å¦‚é¦–å°¾ token æ— æ³•ç›´æ¥äº¤äº’ï¼‰ã€‚
- å®ç°ç®€å•ï¼Œå¸¸ç”¨äºé•¿æ–‡æœ¬æ¨¡å‹çš„å±€éƒ¨æ³¨æ„åŠ›éƒ¨åˆ†ã€‚

![image-20250729145203562](/assets/images/post/image-20250729145203562.png)

#### interleave â€˜fullâ€™ and â€˜LRâ€™ attention

- **'Full' attention**ï¼šå…¨æ³¨æ„åŠ›ï¼ˆå³æ¯ä¸ª token å¯ä»¥çœ‹åˆ°æ•´ä¸ªä¸Šä¸‹æ–‡ï¼‰
- **'LR' attention**ï¼šè¿™é‡Œ LR æŒ‡ **Long-Range**ï¼Œä½†å®é™…æŒ‡çš„æ˜¯ **å—é™æ³¨æ„åŠ›**ï¼Œæ¯”å¦‚ Sliding Window Attention (SWA)ï¼Œç”¨äºå±€éƒ¨ä¸Šä¸‹æ–‡å»ºæ¨¡ã€‚

> **äº¤æ›¿ä½¿ç”¨å…¨æ³¨æ„åŠ›ï¼ˆFull Attentionï¼‰å’Œå±€éƒ¨/å—é™æ³¨æ„åŠ›ï¼ˆå¦‚æ»‘åŠ¨çª—å£æ³¨æ„åŠ› SWAï¼‰â€**ï¼Œä»¥åœ¨**æ•ˆç‡**å’Œ**é•¿è·ç¦»ä¾èµ–å»ºæ¨¡èƒ½åŠ›**ä¹‹é—´å–å¾—å¹³è¡¡ã€‚

From Cohere Command A â€“ Every 4th layer is a full attention

![image-20250729145455753](/assets/images/post/image-20250729145455753.png)

Long-range info via NoPE, short-range info via RoPE + SWA
