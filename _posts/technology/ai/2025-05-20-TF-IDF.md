---
layout: post
title: TF-IDF
tags: ["TF-IDF"]
categories: ["人工智能"]
---

TF-IDF（Term Frequency-Inverse Document Frequency）是一种用于评估一个词在一个文档中的重要性的统计方法。它通过结合词频（TF）和反文档频率（IDF）来衡量词的权重，从而提高文本检索和信息挖掘的准确性。

TF词频（Term Frequency）是指某一个给定的词语在该文件中出现的次数。IDF反文档频率（Inverse Document Frequency）是指如果包含词条的文档越少，IDF越大，则说明词条的类别区分能力越强。

TF-IDF是一种统计方法，用于评估字词或文件的重要程度。例如：

- 在文件集中的字词会随着出现次数的增加呈正比增加趋势。
  
- 在语料库中的文件会随着出现频率的增加呈反比下降趋势。
  

TF-IDF组件基于**词频统计**算法的输出结果（而不是基于原始文档），计算各词语对于各文章的TF-IDF值。

示例代码：

```python
from sklearn.feature_extraction.text import TfidfVectorizer
import jieba
import pandas as pd

documents = [
    "我爱自然语言处理！",
    "自然语言处理很有趣。",
    "我喜欢编程。"
]


# 自定义分词函数
def custom_tokenizer(text):
    return list(jieba.cut(text))


# 初始化TF-IDF向量化器
tfidf = TfidfVectorizer(tokenizer=custom_tokenizer)

# 计算TF-IDF矩阵
tfidf_matrix = tfidf.fit_transform(documents)

# 获取特征名称
feature_names = tfidf.get_feature_names_out()

# 将稀疏矩阵转换为密集矩阵并创建 DataFrame
dense_matrix = tfidf_matrix.toarray()
df = pd.DataFrame(dense_matrix, columns=feature_names)
```

结果：

![image-20250520110104460](/assets/images/post/image-20250520110104460.png)

然后我们自己手动算一下

分词为`['。', '喜欢', '处理', '很', '我', '有趣', '爱', '编程', '自然语言', '！']`

| 分词\出现次数 | 1    | 2    | 3    |
| ------------- | ---- | ---- | ---- |
| 。            | 0    | 1    | 1    |
| 喜欢          | 0    | 0    | 1    |
| 处理          | 1    | 1    | 0    |
| 很            | 0    | 1    | 0    |
| 我            | 1    | 0    | 1    |
| 有趣          | 0    | 1    | 0    |
| 爱            | 1    | 0    | 0    |
| 编程          | 0    | 0    | 1    |
| 自然语言      | 1    | 1    | 0    |
| ！            | 1    | 0    | 0    |

词频矩阵为

| 分词\频率 | 1    | 2    | 3    |
| --------- | ---- | ---- | ---- |
| 。        | 0    | 0.2  | 0.25 |
| 喜欢      | 0    | 0    | 0.25 |
| 处理      | 0.2  | 0.2  | 0    |
| 很        | 0    | 0.2  | 0    |
| 我        | 0.2  | 0    | 0.25 |
| 有趣      | 0    | 0.2  | 0    |
| 爱        | 0.2  | 0    | 0    |
| 编程      | 0    | 0    | 0.25 |
| 自然语言  | 0.2  | 0.2  | 0    |
| ！        | 0.2  | 0    | 0    |

再计算每个关键词对应的逆向文档频率即 IDF 的值$$ln((1+文档总数)/包含关键词 t 的文档数量)$$

| 分词   | IDF  |
| ---- | ---- |
| 。    | 0.69 |
| 喜欢   | 1.38 |
| 处理   | 0.69 |
| 很    | 1.38 |
| 我    | 0.69 |
| 有趣   | 1.38 |
| 爱    | 1.38 |
| 编程   | 1.38 |
| 自然语言 | 0.69 |
| ！    | 1.38 |

然后词频 * IDF

| TF-IDF   | 1     | 2     | 3      |
| -------- | ----- | ----- | ------ |
| 。       | 0     | 0.138 | 0.1725 |
| 喜欢     | 0     | 0     | 0.345  |
| 处理     | 0.138 | 0.138 | 0      |
| 很       | 0     | 0.276 | 0      |
| 我       | 0.138 | 0     | 0.138  |
| 有趣     | 0     | 0.276 | 0      |
| 爱       | 0.276 | 0     | 0      |
| 编程     | 0     | 0     | 0.345  |
| 自然语言 | 0.138 | 0.138 | 0      |
| ！       | 0.276 | 0     | 0      |

擦，没做正规化，但是计算出来结果都是一样的。。。
