---
layout: post
title: 3090-24GB显存vllm部署Qwen3
tags: ["vLLM", "3090", "Qwen"]
categories: ["人工智能"]
---

### 3090-24GB显存vllm部署Qwen3-8B
执行命令
```shell
python -m vllm.entrypoints.openai.api_server \
    --model /ljl/models/Qwen/Qwen3-8B/ \
    --served-model-name qwen3-8b \
    --trust-remote-code \
    --gpu_memory_utilization 0.9 \
    --port 8000 \
    --enable-auto-tool-choice \
    --tool-call-parser hermes \
    --max-num-seqs 4 \
    --max-model-len 8192
```
- --max-num-seqs：设置最大并行处理数量
- --max-model-len：设置最大的token数量

加载模型参数显存占用15.2GB，4 * 8192的tokens KV Cache占用

部署后推理整体显存占用在22-23GB

Qwen3支持开启和关闭推理模式

我用的langchain

在langchain可以这么写关闭推理模式
```python
class Qwen3(ChatOpenAI):
    def __init__(self, streaming=False, **kwargs):
        ChatOpenAI.__init__(
            self,
            streaming=streaming,
            api_key="1234",
            model="qwen3-8b",
            base_url="http://192.168.1.7:9004/v1",
            extra_body={
                "chat_template_kwargs": {"enable_thinking": False}
            },
            **kwargs
        )
```

### 20250520-3090-24GB显存vllm部署Qwen3-30B-A3B-GPTQ-Int4

vllm==0.8.5会报错，降级后正常部署

```shell
# vllm==0.8.4
VLLM_USE_MODELSCOPE=true python -m vllm.entrypoints.openai.api_server \  
    --model /home/li/models/Qwen/Qwen3-30B-A3B-GPTQ-Int4/ \  
    --served-model-name qwen3-30b-a3b-int4 \  
    --trust-remote-code \  
    --gpu_memory_utilization 0.9 \  
    --port 8000 \  
    --enable-reasoning \  
    --reasoning-parser deepseek_r1 \  
    --enable-auto-tool-choice \  
    --tool-call-parser hermes \  
    --max-num-seqs 4 \  
    --max-model-len 4096  

```
