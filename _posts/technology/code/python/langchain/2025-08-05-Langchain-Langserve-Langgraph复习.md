---
layout: post
title: Langchain-Langserve-Langgraphå¤ä¹ 
tags: ["Langchain","Langserve","Langgraph"]
categories: ["äººå·¥æ™ºèƒ½"]
---

## Langchain

### PromptTemplate

æç¤ºè¯æ¨¡æ¿çš„å®ä¾‹åŒ–æ–¹å¼

```python
from langchain_core.prompts import PromptTemplate

# Instantiation using from_template (recommended)
prompt = PromptTemplate.from_template("Say {foo}")
prompt.format(foo="bar")

# Instantiation using initializer
prompt = PromptTemplate(template="Say {foo}")

print(prompt.get_input_schema())
# <class 'langchain_core.utils.pydantic.PromptInput'>

example_prompt = PromptTemplate.from_examples(
    ["hello,world {user}!", "hello,python {user}!"],
    input_variables=["user"],
    example_separator="\n",
    suffix="hello",
    prefix="FUCK"
)

print(example_prompt.format(user="qingchen"))
# FUCK
# hello,world qingchen!
# hello,python qingchen!
# hello

template = PromptTemplate.from_template(
    "hhh{p1}{p2}{p3}",
    partial_variables={"p1": "x", },
)

```

### QingchenModel

è‡ªå·±å°è£…äº†çš„modelæ–¹ä¾¿åˆ‡æ¢æ¨¡å‹

```python
from langchain_openai import ChatOpenAI
from llm_env import get_env


class QingchenModel(ChatOpenAI):
    def __init__(
            self,
            cache=None,
            temperature=0.9,
            verbose=False,
            streaming=False,
            # api_key=get_env.load_env_ali_api_key(),
            # model='qwen-max-latest',
            # base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
            model='moonshot-v1-8k',
            api_key=get_env.load_env_moonshot_api_key(),
            base_url="https://api.moonshot.cn/v1",
            **kwargs
    ):
        ChatOpenAI.__init__(
            self,
            verbose=verbose,
            temperature=temperature,
            streaming=streaming,
            # api_key=get_env.load_env_glm_api_key(),
            api_key=api_key,
            model=model,
            base_url=base_url,
            cache=cache,
            **kwargs
        )
```

### RunnableAssign

å°±æ˜¯æŠŠè¾“å…¥çš„dictå’Œè¿”å›çš„ç»“æœåˆåˆ°ä¸€èµ·

```python
# This is a RunnableAssign
from typing import Dict
from langchain_core.runnables.passthrough import (
    RunnableAssign,
    RunnableParallel,
)
from langchain_core.runnables.base import RunnableLambda
from langchain_community.llms.fake import FakeStreamingListLLM
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import SystemMessagePromptTemplate
from langchain_core.runnables import Runnable
from operator import itemgetter

from langchain_qingchen.chat.QingchenModel import QingchenModel


def add_ten(x: Dict[str, int]) -> Dict[str, int]:
    return {"added": x["input"] + 10}


mapper = RunnableParallel(
    {"add_step": RunnableLambda(add_ten)}
)
print(mapper.invoke({"input": 5}))
# {'add_step': {'added': 15}}

runnable_assign = RunnableAssign(mapper)

# Synchronous example
print(runnable_assign.invoke({"input": 5}))
# returns {'input': 5, 'add_step': {'added': 15}}

# Asynchronous example
# await runnable_assign.ainvoke({"input": 5})
# returns {'input': 5, 'add_step': {'added': 15}}


prompt = (
        SystemMessagePromptTemplate.from_template("You are a nice assistant.")
        + "{question}"
)
print(prompt)
# llm = FakeStreamingListLLM(responses=["foo-lish"])
llm = QingchenModel()

chain: Runnable = prompt | llm | {"str": StrOutputParser()}

print(chain.invoke({"question": "é¡ºä¾¿ç”Ÿæˆä¸€ä¸ªç¬‘è¯"}))

chain_with_assign = chain.assign(hello=itemgetter("str") | llm)
# è¿™é‡Œæ˜¯è¿™æ ·çš„è¾“å…¥æ˜¯{"question": "xxx"} -> {"str": "llm response"} -> ï¼ˆassginä¼šæŠŠè¾“å…¥ä¹Ÿå¸¦ä¸‹æ¥ï¼‰å˜æˆ{"str": "llm response1","hello":"llm response2"}

# print(chain_with_assign.input_schema.model_json_schema())

# print(chain_with_assign.output_schema.model_json_schema())

response = chain_with_assign.invoke({"question": "é¡ºä¾¿ç”Ÿæˆä¸€ä¸ªç¬‘è¯"})
print(response)


# {'add_step': {'added': 15}}
# {'input': 5, 'add_step': {'added': 15}}
# input_variables=['question'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a nice assistant.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})]
# {'str': 'å½“ç„¶å¯ä»¥ï¼Œè¿™é‡Œæœ‰ä¸€ä¸ªå¹½é»˜çš„ç¬‘è¯ä¾›æ‚¨æ¬£èµï¼š\n\næœ‰ä¸€å¤©ï¼Œä¸€åªç‹—èµ°è¿›äº†ä¸€ä¸ªç”µè„‘å•†åº—ï¼Œå¯¹å”®è´§å‘˜è¯´ï¼šâ€œç»™æˆ‘æ¥ä¸¤å°ç”µè„‘ï¼Œä¸€å°ç»™æˆ‘ï¼Œå¦ä¸€å°ç»™æˆ‘çš„çŒ«ã€‚â€\n\nå”®è´§å‘˜æƒŠè®¶åœ°é—®ï¼šâ€œçŒ«ä¹Ÿéœ€è¦ç”µè„‘å—ï¼Ÿâ€\n\nç‹—å›ç­”è¯´ï¼šâ€œæ˜¯çš„ï¼Œæˆ‘çš„çŒ«éå¸¸èªæ˜ï¼Œå®ƒå¯ä»¥ä¸€è¾¹ä¸Šç½‘å†²æµªï¼Œä¸€è¾¹ç ¸æ‰æ‰€æœ‰ä¸å–œæ¬¢å®ƒçš„è€é¼ ã€‚â€\n\nå¸Œæœ›è¿™ä¸ªç¬‘è¯èƒ½è®©æ‚¨å¼€å¿ƒä¸€ç¬‘ï¼'}
# {'str': 'å½“ç„¶å¯ä»¥ï¼Œè¿™é‡Œæœ‰ä¸€ä¸ªè½»æ¾å¹½é»˜çš„ç¬‘è¯ï¼š\n\næœ‰ä¸€å¤©ï¼Œä¸€ä½ç”µè„‘ç¨‹åºå‘˜å»æ‰¾ä»–çš„åŒ»ç”Ÿå¹¶æŠ±æ€¨è¯´ï¼šâ€œåŒ»ç”Ÿï¼Œæˆ‘è§‰å¾—æˆ‘å¾—äº†æŠ‘éƒç—‡ã€‚æˆ‘çœŸçš„å¾ˆä¸å–œæ¬¢æˆ‘çš„å·¥ä½œã€‚â€\nåŒ»ç”Ÿå›ç­”è¯´ï¼šâ€œä¸ºä»€ä¹ˆä¸å°è¯•æ¢ä¸ªå·¥ä½œå‘¢ï¼Ÿæ¯”å¦‚ï¼Œä½ å¯ä»¥å°è¯•å»åŠ¨ç‰©å›­å·¥ä½œã€‚â€\nç¨‹åºå‘˜ç–‘æƒ‘åœ°é—®ï¼šâ€œä½†æ˜¯æˆ‘å¯¹åŠ¨ç‰©ä¸€æ— æ‰€çŸ¥å•Šã€‚â€\nåŒ»ç”Ÿç¬‘ç€è¯´ï¼šâ€œæ²¡å…³ç³»ï¼Œä½ å¯ä»¥å½“ä¸€ä¸ªäººç±»çš„ä¸»é”®ï¼ˆPrimary Keyï¼‰ã€‚â€\n\nç¬‘è¯è§£é‡Šï¼šåœ¨è¿™ä¸ªç¬‘è¯ä¸­ï¼Œâ€œä¸»é”®â€æ˜¯æ•°æ®åº“æœ¯è¯­ï¼ŒæŒ‡çš„æ˜¯ä¸€ä¸ªèƒ½å¤ŸåŒºåˆ†æ•°æ®åº“ä¸­æ¯ä¸ªè®°å½•çš„å”¯ä¸€æ ‡è¯†ç¬¦ã€‚è¿™é‡ŒåŒ»ç”Ÿç”¨äº†ä¸€ä¸ªåŒå…³è¯­ï¼Œæš—ç¤ºç¨‹åºå‘˜è™½ç„¶æ˜¯äººï¼Œä½†å¯¹åŠ¨ç‰©çŸ¥ä¹‹ç”šå°‘ï¼Œæ‰€ä»¥åœ¨åŠ¨ç‰©å›­é‡Œå°±åƒæ˜¯ä¸€ä¸ªç”¨æ¥åŒºåˆ†ä¸åŒäººç±»çš„å”¯ä¸€æ ‡è¯†ç¬¦ï¼Œå³â€œäººç±»çš„ä¸»é”®â€ã€‚', 'hello': AIMessage(content='å“ˆå“ˆï¼Œè¿™ä¸ªç¬‘è¯ç¡®å®å¾ˆæœ‰è¶£ï¼Œå®ƒå·§å¦™åœ°ç»“åˆäº†ç¨‹åºå‘˜çš„ä¸“ä¸šæœ¯è¯­å’Œæ—¥å¸¸ç”Ÿæ´»ï¼Œè®©äººä¼šå¿ƒä¸€ç¬‘ã€‚å¦‚æœä½ æƒ³äº†è§£æ›´å¤šè¿™æ ·çš„ç¬‘è¯ï¼Œæˆ–è€…æœ‰å…¶ä»–é—®é¢˜éœ€è¦å¸®åŠ©ï¼Œéšæ—¶å‘Šè¯‰æˆ‘ï¼', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 39, 'prompt_tokens': 157, 'total_tokens': 196, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'moonshot-v1-8k', 'system_fingerprint': None, 'id': 'chatcmpl-689232111e91f778fae383a1', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--d8b92cf9-9299-40cf-8b94-0c3655d9fd0e-0', usage_metadata={'input_tokens': 157, 'output_tokens': 39, 'total_tokens': 196, 'input_token_details': {}, 'output_token_details': {}})}
```

### RunnableGenerator

æ¥æ”¶ä¸€ä¸ªè¿­ä»£å™¨ï¼Œç„¶åå¯ä»¥æµå¼è¿”å›

```python
# @Author  : ljl
# @Time    : 2025/1/17 ä¸‹åˆ2:44
import asyncio
from typing import Any, AsyncIterator, Iterator
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableGenerator, RunnableLambda
from langchain_core.output_parsers import StrOutputParser

from langchain_qingchen.chat.QingchenModel import QingchenModel


def gen(input: Iterator[Any]) -> Iterator[str]:
    for token in ["Have", " a", " nice", " day"]:
        yield token


runnable = RunnableGenerator(gen)
print(runnable.invoke(None))
print(list(runnable.stream(None)))  # ["Have", " a", " nice", " day"]
print(runnable.batch([None, None]))  # ['Have a nice day', 'Have a nice day']


# Async version:
async def agen(input: AsyncIterator[Any]) -> AsyncIterator[str]:
    for token in ["Have", " a", " nice", " day"]:
        yield token


async def main():
    runnable = RunnableGenerator(agen)
    result = await runnable.ainvoke(None)
    print(result)  # "Have a nice day"
    [print(p) async for p in runnable.astream(None)]  # ["Have", " a", " nice", " day"]


# è¿è¡Œå¼‚æ­¥ä¸»å‡½æ•°
asyncio.run(main())

model = QingchenModel()
chant_chain = (
        ChatPromptTemplate.from_template("Give me a 3 word chant about {topic}")
        | model
        | StrOutputParser()
)


def character_generator(input: Iterator[str]) -> Iterator[str]:
    for token in input:
        if "," in token or "." in token:
            yield "ğŸ‘" + token
        else:
            yield token


runnable = chant_chain | character_generator
assert type(runnable.last) is RunnableGenerator # è¿™é‡Œä¼šè‡ªåŠ¨å°è£…æˆRunnableGenerator
print("".join(runnable.stream({"topic": "waste"})))


# Note that RunnableLambda can be used to delay streaming of one step in a
# sequence until the previous step is finished:
# RunnableLambdaå¯ç”¨äºå»¶è¿Ÿåºåˆ—ä¸­æŸä¸€æ­¥éª¤çš„æµå¼å¤„ç†ï¼Œç›´åˆ°å‰ä¸€æ­¥éª¤å®Œæˆ
def reverse_generator(input: str) -> Iterator[str]:
    # Yield characters of input in reverse order.
    for character in input[::-1]:
        yield character


runnable = chant_chain | RunnableLambda(reverse_generator)
print("".join(runnable.stream({"topic": "waste"})))

# Have a nice day
# ['Have', ' a', ' nice', ' day']
# ['Have a nice day', 'Have a nice day']
# Have a nice day
# Have
#  a
#  nice
#  day
# ReduceğŸ‘, ReuseğŸ‘, Recycle
# elcyceR ,esueR ,ecudeR
```

### RunnableLambda

æˆ‘å¹³æ—¶å†™ç”¨çš„å¾ˆå¤šï¼Œå¯ä»¥å†™ä¸€ä¸ªæ–¹æ³•ç”¨RunnableLambdaåŒ…ä¸€ä¸‹å°±èƒ½ç›´æ¥ç”¨ä¸Š

```python
from langchain_core.runnables import RunnableLambda
from langchain_core.globals import set_debug
from langchain_core.tracers import ConsoleCallbackHandler
import random

set_debug(True)


def add_one(x: int) -> int:
    return x + 1


def buggy_double(y: int) -> int:
    """Buggy code that will fail 70% of the time"""
    if random.random() > 0.3:
        print('This code failed, and will probably be retried!')  # noqa: T201
        raise ValueError('Triggered buggy code')
    return y * 2


sequence = (
        RunnableLambda(add_one) |
        RunnableLambda(buggy_double).with_retry(  # Retry on failure
            stop_after_attempt=10,
            wait_exponential_jitter=False
        )
)

print(sequence.input_schema.model_json_schema())  # Show inferred input schema
print(sequence.output_schema.model_json_schema())  # Show inferred output schema
# print(sequence.invoke(2, config={'callbacks': [ConsoleCallbackHandler()]}))  # invoke the sequence (note the retry above!!)
print(sequence.invoke(2))  # invoke the sequence (note the retry above!!)

# {'title': 'add_one_input', 'type': 'integer'}
# {'title': 'buggy_double_output', 'type': 'integer'}
# [chain/start] [chain:RunnableSequence] Entering Chain run with input:
# {
#   "input": 2
# }
# [chain/start] [chain:RunnableSequence > chain:add_one] Entering Chain run with input:
# {
#   "input": 2
# }
# [chain/end] [chain:RunnableSequence > chain:add_one] [0ms] Exiting Chain run with output:
# {
#   "output": 3
# }
# [chain/start] [chain:RunnableSequence > chain:buggy_double] Entering Chain run with input:
# {
#   "input": 3
# }
# [chain/start] [chain:RunnableSequence > chain:buggy_double > chain:buggy_double] Entering Chain run with input:
# {
#   "input": 3
# }
# [chain/end] [chain:RunnableSequence > chain:buggy_double > chain:buggy_double] [0ms] Exiting Chain run with output:
# {
#   "output": 6
# }
# [chain/end] [chain:RunnableSequence > chain:buggy_double] [1ms] Exiting Chain run with output:
# {
#   "output": 6
# }
# [chain/end] [chain:RunnableSequence] [2ms] Exiting Chain run with output:
# {
#   "output": 6
# }
# 6

```

### RunnableParallel

å¹¶è¡Œ

```python
from langchain_core.runnables import RunnableLambda, RunnableParallel
from langchain_core.prompts import ChatPromptTemplate

from langchain_qingchen.chat.QingchenModel import QingchenModel


def add_one(x: int) -> int:
    return x + 1


def mul_two(x: int) -> int:
    return x * 2


def mul_three(x: int) -> int:
    return x * 3


runnable_1 = RunnableLambda(add_one)
runnable_2 = RunnableLambda(mul_two)
runnable_3 = RunnableLambda(mul_three)

sequence = runnable_1 | {  # this dict is coerced to a RunnableParallel
    "mul_two": runnable_2,
    "mul_three": runnable_3,
}
# Or equivalently:
# sequence = runnable_1 | RunnableParallel(
#     {"mul_two": runnable_2, "mul_three": runnable_3}
# )
# Also equivalently:
# sequence = runnable_1 | RunnableParallel(
#     mul_two=runnable_2,
#     mul_three=runnable_3,
# )

print(sequence.invoke(1))
# await sequence.ainvoke(1)

print(sequence.batch([1, 2, 3]))
# await sequence.abatch([1, 2, 3])


model = QingchenModel()
joke_chain = (
        ChatPromptTemplate.from_template("ç»™æˆ‘è®²ä¸ªå…³äº{topic}çš„ç¬‘è¯")
        | model
)
poem_chain = (
        ChatPromptTemplate.from_template("å†™ä¸€é¦–å…³äº{topic}çš„ä¸¤è¡Œè¯—")
        | model
)

runnable = RunnableParallel(joke=joke_chain, poem=poem_chain)

# Display stream
# output = {key: "" for key, _ in runnable.output_schema()}
for chunk in runnable.stream({"topic": "æ³³è¡£"}):
    for key in chunk:
        print(chunk)  # noqa: T201

# {'mul_two': 4, 'mul_three': 6}
# [{'mul_two': 4, 'mul_three': 6}, {'mul_two': 6, 'mul_three': 9}, {'mul_two': 8, 'mul_three': 12}]
# {'joke': AIMessageChunk(content='', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='å¥½çš„', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='ï¼Œ', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='è¿™é‡Œ', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='æœ‰ä¸€ä¸ª', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='å…³äº', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='æ³³', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='è¡£', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='çš„', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='è½»æ¾', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='å¹½é»˜', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='çš„', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='ç¬‘è¯', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='ï¼š\n\n', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='æœ‰ä¸€å¤©', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='ï¼Œ', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='ä¸€ä½', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='é¡¾å®¢', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='èµ°è¿›', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='æ³³', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='è¡£', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='åº—', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='å¯¹', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='åº—å‘˜', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='è¯´', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='ï¼šâ€œ', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='æˆ‘æƒ³', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='æ‰¾', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='ä¸€å¥—', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='æ—¢', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='æ—¶å°š', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='åˆ', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='å®ç”¨çš„', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='æ³³', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='è¡£', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='ã€‚', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='â€\n', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='åº—å‘˜', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='å›ç­”', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='ï¼šâ€œ', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='å½“ç„¶', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='ï¼Œ', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='æˆ‘ä»¬', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='è¿™é‡Œæœ‰', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='å„ç§å„æ ·çš„', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='æ³³', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='è¡£', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='ã€‚', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='â€\n', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='é¡¾å®¢', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='è¯´', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='ï¼šâ€œ', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='æˆ‘æƒ³è¦', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='ä¸€æ¬¾', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='å³ä½¿', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='åœ¨æ°´ä¸­', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='ä¹Ÿä¸ä¼š', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='å˜å¾—', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='é€æ˜çš„', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='æ³³', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='è¡£', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='ã€‚', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='â€\n', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='åº—å‘˜', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='æƒ³äº†æƒ³', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='ï¼Œ', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='ç„¶å', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='å¾®ç¬‘ç€', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='è¯´', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='ï¼šâ€œ', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='é‚£', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='æ‚¨', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='å¯èƒ½', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='éœ€è¦', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='ä¸€æ¬¾', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='æ½œæ°´', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='æœ', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='ã€‚â€\n\n', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='è¿™ä¸ª', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='ç¬‘è¯', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='ä»¥', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='æ³³', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='è¡£', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='çš„ææ–™', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='å’Œ', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='åŠŸèƒ½', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='ä¸º', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='åˆ‡å…¥ç‚¹', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='ï¼Œ', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='ç”¨', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='ä¸€ä¸ª', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='å¹½é»˜', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='çš„', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='è½¬æŠ˜', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='æ¥', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='è¾¾åˆ°', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='å¹½é»˜', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='æ•ˆæœ', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='ã€‚', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='å¸Œæœ›', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='è¿™ä¸ª', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='ç¬‘è¯', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='èƒ½', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='è®©æ‚¨', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='ä¼š', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='å¿ƒ', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='ä¸€ç¬‘', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='ï¼', additional_kwargs={}, response_metadata={}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'joke': AIMessageChunk(content='', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'moonshot-v1-8k', 'system_fingerprint': 'fpv0_ff52a3ef'}, id='run--b900033d-941d-426c-8359-fda5f825cbf9')}
# {'poem': AIMessageChunk(content='', additional_kwargs={}, response_metadata={}, id='run--a998b997-aca5-4de9-9281-dd63d96d89ad')}
# {'poem': AIMessageChunk(content='æ°´ä¸­', additional_kwargs={}, response_metadata={}, id='run--a998b997-aca5-4de9-9281-dd63d96d89ad')}
# {'poem': AIMessageChunk(content='èˆ', additional_kwargs={}, response_metadata={}, id='run--a998b997-aca5-4de9-9281-dd63d96d89ad')}
# {'poem': AIMessageChunk(content='è€…', additional_kwargs={}, response_metadata={}, id='run--a998b997-aca5-4de9-9281-dd63d96d89ad')}
# {'poem': AIMessageChunk(content='æŠ«', additional_kwargs={}, response_metadata={}, id='run--a998b997-aca5-4de9-9281-dd63d96d89ad')}
# {'poem': AIMessageChunk(content='è½»', additional_kwargs={}, response_metadata={}, id='run--a998b997-aca5-4de9-9281-dd63d96d89ad')}
# {'poem': AIMessageChunk(content='çº±', additional_kwargs={}, response_metadata={}, id='run--a998b997-aca5-4de9-9281-dd63d96d89ad')}
# {'poem': AIMessageChunk(content='ï¼Œ\n', additional_kwargs={}, response_metadata={}, id='run--a998b997-aca5-4de9-9281-dd63d96d89ad')}
# {'poem': AIMessageChunk(content='ç¢§', additional_kwargs={}, response_metadata={}, id='run--a998b997-aca5-4de9-9281-dd63d96d89ad')}
# {'poem': AIMessageChunk(content='æ³¢', additional_kwargs={}, response_metadata={}, id='run--a998b997-aca5-4de9-9281-dd63d96d89ad')}
# {'poem': AIMessageChunk(content='æ˜ ', additional_kwargs={}, response_metadata={}, id='run--a998b997-aca5-4de9-9281-dd63d96d89ad')}
# {'poem': AIMessageChunk(content='æ—¥', additional_kwargs={}, response_metadata={}, id='run--a998b997-aca5-4de9-9281-dd63d96d89ad')}
# {'poem': AIMessageChunk(content='æ³³', additional_kwargs={}, response_metadata={}, id='run--a998b997-aca5-4de9-9281-dd63d96d89ad')}
# {'poem': AIMessageChunk(content='è¡£', additional_kwargs={}, response_metadata={}, id='run--a998b997-aca5-4de9-9281-dd63d96d89ad')}
# {'poem': AIMessageChunk(content='å', additional_kwargs={}, response_metadata={}, id='run--a998b997-aca5-4de9-9281-dd63d96d89ad')}
# {'poem': AIMessageChunk(content='ã€‚', additional_kwargs={}, response_metadata={}, id='run--a998b997-aca5-4de9-9281-dd63d96d89ad')}
# {'poem': AIMessageChunk(content='', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'moonshot-v1-8k', 'system_fingerprint': 'fpv0_ff52a3ef'}, id='run--a998b997-aca5-4de9-9281-dd63d96d89ad')}
```

### RunnablePassthrough

è¿™ä¸ªç”¨çš„ä¹Ÿè›®å¤šçš„

ç»å¸¸è¿™ä¹ˆå†™ï¼š

```python
RunnablePassthrough.assign(test=lambda x: len(x))
# teståé¢æ˜¯ä¸ªrunnableçš„
# RunnablePassthroughä¼šæŠŠå‰é¢å­—å…¸å¸¦è¿›æ¥,assignæŠŠç»“æœåŠ è¿›å»ï¼Œä¹Ÿå°±æ˜¯è¯´å¯ä»¥æŠŠç¬¬ä¸€æ­¥çš„æ¨¡å‹è¿”å›å¸¦åˆ°ç¬¬äºŒæ­¥æœ€ç»ˆä¸€èµ·è¿”å›
```

```python
import time

from langchain.globals import set_debug
from langchain_core.runnables import (
    RunnableLambda,
    RunnableParallel,
    RunnablePassthrough,
)

from langchain_qingchen.chat.QingchenModel import QingchenModel


# set_debug(True)
runnable = RunnableParallel(
    origin=RunnablePassthrough(),
    modified=lambda x: x + 1
)

print(runnable.invoke(1))


def fake_llm(prompt: str) -> str:  # Fake LLM for the example
    return "completion"


chain = RunnableLambda(fake_llm) | {
    'original': RunnablePassthrough(),  # Original LLM output
    'parsed': lambda text: text[::-1]  # Parsing logic
}

print(chain.invoke('hello'))

runnable = {
               'llm1': fake_llm,
               'llm2': fake_llm,
           } | RunnablePassthrough.assign(
    total_chars=lambda inputs: len(inputs['llm1'] + inputs['llm2'])
)

print(runnable.invoke('hello'))
# {'llm1': 'completion', 'llm2': 'completion', 'total_chars': 20}


# def time_wait(input):
#     print("sleeping")
#     time.sleep(2)
#
#
# def print_res(inputs):
#     print(inputs)
#
#
# chain2 = QingchenModel() | RunnablePassthrough(time_wait) | RunnablePassthrough(print_res)
# # print(chain2.invoke('ä½ å¥½'))
#
# messages = [
#     ("human", "å¼€å§‹"),
#     ("ai", "Hello, I am Bulbasaur. What's your name?"),
#     ("human", "Hello. My name is Pikachu."),
#     ("ai", "Nice to meet you, Pikachu."),
#     ("human", "What are you doing?"),
#     ("ai", "I'm exploring this area. What about you?"),
#     ("human", "I'm learning English")
# ]
# # print(chain2.invoke(messages))
# for e in chain2.stream(messages):
#     print(e)

# {'origin': 1, 'modified': 2}
# {'original': 'completion', 'parsed': 'noitelpmoc'}
# {'llm1': 'completion', 'llm2': 'completion', 'total_chars': 20}

```

### RunnableWithMessageHistory

å¸¦å¯¹è¯è®°å¿†å†å²é€šè¿‡sessionåŒºåˆ†ä¸åŒçš„äºº

```python
from typing import List
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.messages import BaseMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from pydantic import BaseModel, Field
from langchain_core.runnables import (
    ConfigurableFieldSpec,
)
from langchain_core.runnables.history import RunnableWithMessageHistory

from langchain_qingchen.chat.QingchenModel import QingchenModel


class InMemoryHistory(BaseChatMessageHistory, BaseModel):
    """In memory implementation of chat message history."""

    messages: List[BaseMessage] = Field(default_factory=list)

    def add_messages(self, messages: List[BaseMessage]) -> None:
        """Add a list of messages to the store"""
        self.messages.extend(messages)

    def clear(self) -> None:
        self.messages = []


# Here we use a global variable to store the chat message history.
# This will make it easier to inspect it to see the underlying results.
store = {}


def get_by_session_id(session_id: str) -> BaseChatMessageHistory:
    if session_id not in store:
        store[session_id] = InMemoryHistory()
    return store[session_id]


history = get_by_session_id("1")
history.add_message(AIMessage(content="hello"))
print(store)  # noqa: T201

prompt = ChatPromptTemplate.from_messages([
    ("system", "You're an assistant who's good at {ability}"),
    MessagesPlaceholder(variable_name="history"),
    ("human", "{question}"),
])

chain = prompt | QingchenModel()

chain_with_history = RunnableWithMessageHistory(
    chain,
    get_by_session_id,
    input_messages_key="question",
    history_messages_key="history",
)

print(chain_with_history.invoke(  # noqa: T201
    {"ability": "math", "question": "What does cosine mean?"},
    config={"configurable": {"session_id": "foo"}}
))

# Uses the store defined in the example above.
print(store)  # noqa: T201

print(chain_with_history.invoke(  # noqa: T201
    {"ability": "math", "question": "What's its inverse"},
    config={"configurable": {"session_id": "foo"}}
))

print(store)  # noqa: T201


store = {}


def get_session_history(
        user_id: str, conversation_id: str
) -> BaseChatMessageHistory:
    if (user_id, conversation_id) not in store:
        store[(user_id, conversation_id)] = InMemoryHistory()
    return store[(user_id, conversation_id)]


prompt = ChatPromptTemplate.from_messages([
    ("system", "You're an assistant who's good at {ability}"),
    MessagesPlaceholder(variable_name="history"),
    ("human", "{question}"),
])

chain = prompt | QingchenModel()

with_message_history = RunnableWithMessageHistory(
    chain,
    get_session_history=get_session_history,
    input_messages_key="question",
    history_messages_key="history",
    history_factory_config=[
        ConfigurableFieldSpec(
            id="user_id",
            annotation=str,
            name="User ID",
            description="Unique identifier for the user.",
            default="",
            is_shared=True,
        ),
        ConfigurableFieldSpec(
            id="conversation_id",
            annotation=str,
            name="Conversation ID",
            description="Unique identifier for the conversation.",
            default="",
            is_shared=True,
        ),
    ],
)

with_message_history.invoke(
    {"ability": "math", "question": "What does cosine mean?"},
    config={"configurable": {"user_id": "123", "conversation_id": "1"}}
)
```

### FakeModel

##### å¾…è¡¥å……

ä¼ªé€ ä¸€ä¸ªå¤§æ¨¡å‹è¿”å›æ¥åšæµ‹è¯•

```python
def fake_model(input) -> Iterator[ChatGenerationChunk]:
    print("into fake_model")
    for c in content:
        yield AIMessageChunk(c)
        
full_chain = ((RunnableLambda(fake_model) | JsonOutputParser()).with_retry(stop_after_attempt=3)
              | RunnablePassthrough.assign(
            test_summary=itemgetter("input") | RunnableLambda(lambda x: x)
        ))
```

### CustomJsonOutputParser

`JsonOutputParser`å®é™…ä½¿ç”¨ä¸­æœ‰ä¸€ä¸ªé—®é¢˜å¦‚æœ`json`æ ¼å¼æœ‰é—®é¢˜ç›´æ¥å°±è¿”å›`none`äº†

æˆ‘ä»¬å¯ä»¥ç»§æ‰¿ç„¶åæ”¹ä¸€ä¸‹

```python
import random
from json import JSONDecodeError
from typing import Iterator, Optional, Callable, Any
from langchain_core.messages import AIMessageChunk
from langchain_core.outputs import ChatGenerationChunk, Generation
from langchain_core.runnables import RunnableLambda
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.utils.json import parse_json_markdown

content1 = """{\n    "target": "è¿›è¡Œç®€å•çš„è‹±è¯­å¯¹è¯äº¤æµ",\n    "roles": [\n        "Bulbasaur",\n        "Pikachu"\n    ],\n    "link": [\n        "ç¯èŠ‚1:Bulbasaurï¼šâ€˜Hello, I am Bulbasaur. What\'s your name?â€™  Pikachuï¼šâ€˜Hello. My name is Pikachu.â€™",\n        "ç¯èŠ‚2:Bulbasaurï¼šâ€˜Nice to meet you, Pikachu.â€™  Pikachuï¼šâ€˜Nice to meet you too.â€™",\n        "ç¯èŠ‚3:Bulbasaurï¼šâ€˜I\'m exploring this area. What about you?â€™  Pikachuï¼šâ€˜What are you doing?â€™",\n        "ç»“æŸç¯èŠ‚:Bulbasaurï¼šâ€˜Well, see you next time! <ç»“æŸ>â€™  Pikachuï¼šâ€˜byeâ€™"\n    ],\n    "scene": "In a lush forest, Bulbasaur and Pikachu meet. Bulbasaur is looking around curiously when he sees Pikachu and says:",\n    "actor_line": "Well, see you next time!",\n    "prompt": "Bulbasaur in a forest, looking a bit disappointed, high quality"\n} """

content2 = """how are you?ä½ å¥½å—ï¼Ÿ"""


def fake_model(input) -> Iterator[ChatGenerationChunk]:
    print("into fake_model")
    # éšæœºé€‰æ‹©ä¸€ä¸ªcontent
    content = content1 if random.random() > 0.5 else content2
    for c in content:
        yield AIMessageChunk(c)


test_chain = fake_model | JsonOutputParser()
# print(test_chain.invoke("hello"))


# è‡ªå®šä¹‰è¾“å‡ºè§£æå™¨
class CustomJsonOutputParser(JsonOutputParser):
    """
    è‡ªå®šä¹‰JSONè¾“å‡ºè§£æå™¨ï¼Œæ”¯æŒJSONè§£æå’Œè‡ªå®šä¹‰å›é€€é€»è¾‘
    """

    def __init__(
            self,
            pydantic_object: Optional[Any] = None,
            fallback_func: Optional[Callable[[str], dict]] = None
    ):
        """
        åˆå§‹åŒ–è‡ªå®šä¹‰JSONè¾“å‡ºè§£æå™¨

        :param pydantic_object: å¯é€‰çš„Pydanticæ¨¡å‹
        :param fallback_func: å½“æ— æ³•è§£æJSONæ—¶çš„è‡ªå®šä¹‰å¤„ç†å‡½æ•°
        """
        # ä½¿ç”¨çˆ¶ç±»æ„é€ å‡½æ•°
        super().__init__(pydantic_object=pydantic_object)
        # å­˜å‚¨å›é€€å‡½æ•°
        self._fallback_func = fallback_func

    def parse_result(self, result: list[Generation], *, partial: bool = False) -> Any:
        text = result[0].text
        text = text.strip()
        if partial:
            try:
                return parse_json_markdown(text)
            except JSONDecodeError:
                # ä¸æ˜¯jsonæ ¼å¼è¿”å›æ—¶
                return self._fallback_func(text)
        else:
            try:
                return parse_json_markdown(text)
            except JSONDecodeError as e:
                # msg = f"Invalid json output: {text}"
                return self._fallback_func(text)


# è‡ªå®šä¹‰å›é€€å‡½æ•°ç¤ºä¾‹
def custom_fallback(text: str) -> dict:
    """
    è‡ªå®šä¹‰å›é€€å¤„ç†å‡½æ•°

    :param text: åŸå§‹æ–‡æœ¬
    :return: å¤„ç†åçš„å­—å…¸
    """
    return {
        "original_text": text,
        "processed": text.upper(),
        "length": len(text),
        "parse_status": "custom_fallback"
    }


# åˆ›å»ºè§£æå™¨å®ä¾‹
parser = CustomJsonOutputParser(fallback_func=custom_fallback)
chain = RunnableLambda(fake_model) | parser

# for e in chain.stream("ç”¨jsonæ ¼å¼è¾“å‡ºï¼š{'a':1}"):
#     # print(type(e))
#     print(e)

print(chain.invoke("ç”¨jsonæ ¼å¼è¾“å‡ºï¼š{'a':1}"))
```

### å¾…è¡¥å……

## Langserve

é€šè¿‡fastapiåŠ ä¸Šlangserveå¯ä»¥å¿«é€Ÿæ­å»ºå·¥ä½œæµ

```python
#!/usr/bin/env python
from fastapi import FastAPI
from langchain.prompts import ChatPromptTemplate
from langserve import add_routes, RemoteRunnable
from pydantic import BaseModel

from langchain_qingchen.chat.QingchenModel import QingchenModel


class InputClass(BaseModel):
    userinfo: dict


app = FastAPI(
    title="LangChain Server",
    version="1.0",
    description="A simple api server using Langchain's Runnable interfaces",
)

add_routes(
    app,
    QingchenModel(),
    path="/qingchen",
)

add_routes(
    app,
    RemoteRunnable("http://192.168.1.9:8000/joke/").with_types(input_type=InputClass),
    path="/new/joke"
)

model = QingchenModel()
prompt = ChatPromptTemplate.from_template("tell me a joke about {topic}")
add_routes(
    app,
    prompt | model,
    path="/joke",
)

if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="localhost", port=8000)
```

### å¾…è¡¥å……

## Langgraph

### å¿«é€Ÿæ­å»º

```python
# @Author  : ljl
# @Time    : 2025/1/25 ä¸‹åˆ3:30
# Import relevant functionality
from datetime import datetime

from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.messages import HumanMessage
from langchain_core.tools import tool
from langgraph.checkpoint.memory import MemorySaver
from langgraph.prebuilt import create_react_agent

from langchain_qingchen.chat.QingchenModel import QingchenModel

# Create the agent
memory = MemorySaver()
model = QingchenModel()


@tool
def get_time():
    """
    è·å–å½“å‰æ—¶é—´
    :return: å½“å‰æ—¶é—´
    """
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")


@tool
def get_location():
    """
    è·å–ç”¨æˆ·å½“å‰ä½ç½®
    :return: ç”¨æˆ·å½“å‰ä½ç½®
    """
    return 'å—äº¬å¸‚æµ¦å£åŒº'


tools = [get_time, get_location]
agent_executor = create_react_agent(model, tools, checkpointer=memory)

# Use the agent
config = {"configurable": {"thread_id": "abc123"}}
for chunk in agent_executor.stream(
        {"messages": [HumanMessage(content="æˆ‘æ˜¯æ¸…å°˜ï¼Œæˆ‘ç°åœ¨åœ¨å“ªï¼Ÿ")]}, config
):
    print(chunk)
    print("----")

for chunk in agent_executor.stream(
        {"messages": [HumanMessage(content="å—äº¬ç°åœ¨å‡ ç‚¹äº†")]}, config
):
    print(chunk)
    print("----")


# {'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': '01987b44c25a51af954a59a546322a55', 'function': {'arguments': '{}', 'name': 'get_location'}, 'type': 'function', 'index': 0}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 2, 'prompt_tokens': 131, 'total_tokens': 133, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'deepseek-ai/DeepSeek-V3', 'system_fingerprint': '', 'id': '01987b44bcbf607a03000fd17ff8b722', 'service_tier': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--1e85b1da-2476-4034-9258-c05af6e8e796-0', tool_calls=[{'name': 'get_location', 'args': {}, 'id': '01987b44c25a51af954a59a546322a55', 'type': 'tool_call'}], usage_metadata={'input_tokens': 131, 'output_tokens': 2, 'total_tokens': 133, 'input_token_details': {}, 'output_token_details': {}})]}}
# ----
# {'tools': {'messages': [ToolMessage(content='å—äº¬å¸‚æµ¦å£åŒº', name='get_location', id='499460a1-383e-4df2-a6e8-33a73af37bc9', tool_call_id='01987b44c25a51af954a59a546322a55')]}}
# ----
# {'agent': {'messages': [AIMessage(content='æ¸…å°˜ï¼Œä½ ç°åœ¨çš„ä½ç½®æ˜¯åœ¨å—äº¬å¸‚æµ¦å£åŒºã€‚', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 156, 'total_tokens': 167, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'deepseek-ai/DeepSeek-V3', 'system_fingerprint': '', 'id': '01987b44c4766cd3fe401958ff480d4b', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--74a43175-bca4-4b10-81f3-14d9fdf6b2dd-0', usage_metadata={'input_tokens': 156, 'output_tokens': 11, 'total_tokens': 167, 'input_token_details': {}, 'output_token_details': {}})]}}
# ----
# {'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': '01987b44d9c94236ea31c9ab626358b5', 'function': {'arguments': '{}', 'name': 'get_time'}, 'type': 'function', 'index': 0}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 2, 'prompt_tokens': 174, 'total_tokens': 176, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'deepseek-ai/DeepSeek-V3', 'system_fingerprint': '', 'id': '01987b44cfca41732375fcabbdd2e0ff', 'service_tier': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--5a140a15-728c-4c7d-a576-1546c126f236-0', tool_calls=[{'name': 'get_time', 'args': {}, 'id': '01987b44d9c94236ea31c9ab626358b5', 'type': 'tool_call'}], usage_metadata={'input_tokens': 174, 'output_tokens': 2, 'total_tokens': 176, 'input_token_details': {}, 'output_token_details': {}})]}}
# ----
# {'tools': {'messages': [ToolMessage(content='2025-08-06 01:26:07', name='get_time', id='a4b538c2-780c-401c-89e6-2c255e5ac4f8', tool_call_id='01987b44d9c94236ea31c9ab626358b5')]}}
# ----
# {'agent': {'messages': [AIMessage(content='å—äº¬ç°åœ¨æ˜¯2025å¹´8æœˆ6æ—¥å‡Œæ™¨1ç‚¹26åˆ†ã€‚', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 207, 'total_tokens': 221, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'deepseek-ai/DeepSeek-V3', 'system_fingerprint': '', 'id': '01987b44dc553d3469e67f211c2c2e9d', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--5a662040-3c10-491f-bd04-a3c203f38b25-0', usage_metadata={'input_tokens': 207, 'output_tokens': 14, 'total_tokens': 221, 'input_token_details': {}, 'output_token_details': {}})]}}
# ----
```

### åˆ†æ­¥æ­å»º

#### å·¥å…·

ä¸»è¦æ˜¯ç”¨`convert_to_openai_tool`è½¬æˆå¯¹åº”çš„æ ¼å¼

```python
# @Time    : 2025/1/25 ä¸‹åˆ2:12
from datetime import datetime
from langchain_core.tools import tool
from typing import List
from langchain_core.utils.function_calling import convert_to_openai_tool
from utils.bingsearch import run_bingsearch_workflow


# 20250325 ä¼˜åŒ–å›¾ç‰‡æœç´¢å·¥å…·
@tool
def image_bing_search(keyword: str, count: int, offset: int) -> List[str]:
    """
    ä½¿ç”¨bingæœå›¾å·¥å…·
    :param keyword: æœå›¾æç¤ºè¯
    :param count: æœç´¢ç»“æœæ•°é‡
    :param offset: æœç´¢ç»“æœåç§»é‡ï¼Œç”¨äºç¿»é¡µï¼Œé»˜è®¤ä¸º0-30çš„éšæœºæ•´æ•°
    :return: æœå›¾ç»“æœæ˜¯å›¾ç‰‡é“¾æ¥åˆ—è¡¨
    """
    import random
    if not offset:
        offset = random.randint(0, 30)
    return run_bingsearch_workflow(keyword, count, offset)


@tool
def current_time() -> str:
    """
    è·å–å½“å‰æ—¶é—´
    :return: å½“å‰æ—¶é—´
    """
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")


image_bing_search_tool = convert_to_openai_tool(image_bing_search)
current_time_tool = convert_to_openai_tool(current_time)

tools = [image_bing_search, current_time]
TOOLS = [image_bing_search_tool, current_time_tool]
```

#### æ¨¡å‹

å…³é”®åœ¨äº`bind`ç»‘å®šå·¥å…·

```python
# @Time    : 2025/2/8 ä¸Šåˆ10:02
from assistant.tools import TOOLS
from langchain_qingchen.chat.QingchenModel import QingchenModel

llm_with_tools = QingchenModel(tags=["show", "tools", "agent"]).bind(tools=TOOLS)
```

#### æ„å»ºå›¾

```python
# @Author  : ljl
# @Time    : 2025/1/25 ä¸‹åˆ1:48
# æ­£å¸¸å¯¹è¯åŒ…å«å†å²è®°å½•ç¼“å­˜ï¼ŒåŒ…å«ä¸€ä¸ªæœå›¾çš„å·¥å…·
# langgraphå®ç°
from typing import Annotated
from langchain_core.messages import BaseMessage, ToolMessage, AIMessageChunk, SystemMessage
from langgraph.constants import END
from langgraph.prebuilt import ToolNode, tools_condition
from typing_extensions import TypedDict
from langgraph.graph import StateGraph, START
from langgraph.graph.message import add_messages
from langgraph.checkpoint.memory import MemorySaver
from assistant.assistant_constants import *
from assistant.model_with_tools import llm_with_tools
from assistant.quick_instructions import quick_instructions_pic, quick_instructions_exercise, quick_instructions_advice, quick_instructions_lesson
from assistant.tools import tools
from utils.mylog import logger


# å…¥å‚
class AssistantState(TypedDict):
    messages: Annotated[list, add_messages]
    thread_uuid: Annotated[str, "ä¸´æ—¶å¯¹è¯çº¿ç¨‹UUID"]
    quick_instructions: Annotated[bool, "æ˜¯å¦æ˜¯å¿«æ·æŒ‡ä»¤"]


# å…¥å£
def entry(state: AssistantState):
    logger.info(f"AI Assistant Entry: {state}")


# æ¨¡å‹
def chatbot(state: AssistantState):
    dialogue: list[BaseMessage] = state["messages"]
    # æ·»åŠ ç³»ç»Ÿæç¤ºè¯
    dialogue.insert(0, SystemMessage(content=system_prompt))
    return {"messages": [llm_with_tools.invoke(dialogue)]}


def get_last_message(state: AssistantState) -> BaseMessage:
    if messages := state.get("messages", []):
        message: BaseMessage = messages[-1]
    else:
        raise ValueError(f"æ²¡æœ‰æ”¶åˆ°ç”¨æˆ·æ¶ˆæ¯: {state}")
    return message


# å¿«æ·æŒ‡ä»¤
def quick_instructions(state: AssistantState):
    message = get_last_message(state)
    logger.info(f"AI Assistant Quick Instructions: {message}")
    lesson_content = state["lesson_content"]
    if message.content == Q1: 
        return {"messages": [quick_instructions_pic(lesson_content)], "quick_instructions": False}
    elif message.content == Q2: 
        return {"messages": [quick_instructions_exercise(lesson_content)], "quick_instructions": False}
    elif message.content == Q3: 
        return {"messages": [quick_instructions_advice(lesson_content)], "quick_instructions": False}
    elif Q4 in message.content: 
        return {"messages": [quick_instructions_lesson(lesson_content)], "quick_instructions": False}
    else:
        return {"messages": [AIMessageChunk(content="è§¦å‘äº†å¿«æ·æŒ‡ä»¤ï¼Œä½†æœªæ‰¾åˆ°å¯¹åº”çš„æŒ‡ä»¤")], "quick_instructions": False}


# å…¥å£è·¯ç”±
def entry_route(state: AssistantState):
    """
    å¿«æ·æŒ‡ä»¤åˆ¤æ–­è·¯ç”±ï¼ŒåŒé‡æ ¡éªŒ
    """
    message = get_last_message(state)

    # ç¬¬ä¸€å±‚æ ¡éªŒ
    if state.get("quick_instructions", False):
        # ç¬¬äºŒå±‚æ ¡éªŒ
        if trigger_word in message.content:
            # å¤„ç†å¿«æ·æŒ‡ä»¤
            return "quick"
    return "chatbot"


# å›¾
def build_graph():
    # tool
    tool_node = ToolNode(tools=tools)

    # memory
    memory = MemorySaver()

    # build graph
    graph_builder = StateGraph(AssistantState)

    graph_builder.add_node("entry", entry)
    graph_builder.add_node("quick", quick_instructions)
    graph_builder.add_node("chatbot", chatbot)
    graph_builder.add_node("tools", tool_node)

    graph_builder.add_conditional_edges(
        "entry",
        entry_route,
        {"chatbot": "chatbot", "quick": "quick"},
    )

    graph_builder.add_conditional_edges(
        "chatbot",
        tools_condition
    )

    graph_builder.add_edge(START, "entry")
    graph_builder.add_edge("tools", "chatbot")
    graph_builder.add_edge("quick", END)

    agent_graph = graph_builder.compile(checkpointer=memory)
    return agent_graph


graph = build_graph().with_config({"run_name": "ai_assistant"})


# æµå¼
async def assistant_chat_stream(inputs: AssistantState):
    config = {"configurable": {"thread_id": inputs.get("thread_uuid")}}
    async for message, metadata in graph.astream(inputs, config, stream_mode="messages"):
        message: BaseMessage = message
        if "show" not in metadata.get("tags", []):
            # logger.info(f"AI Assistant Hide Response: {message}")
            continue
        if isinstance(message, ToolMessage):
            logger.info(f"AI Assistant Tool Message: {message}")
            continue
        elif isinstance(message, AIMessageChunk):
            if message.tool_calls:
                logger.info(f"AI Assistant Tool Calls: {message}")
                continue
        yield message

# å±•ç¤ºgraphç»“æ„
# def show_graph():
#     from PIL import Image
#     from io import BytesIO
#     import matplotlib.pyplot as plt
#     from langchain_core.runnables.graph import MermaidDrawMethod
#
#     def show_img(path) -> None:
#         img = Image.open(path)
#         plt.axis('off')  # ä¸æ˜¾ç¤ºåæ ‡è½´
#         plt.imshow(img)  # å°†æ•°æ®æ˜¾ç¤ºä¸ºå›¾åƒï¼Œå³åœ¨äºŒç»´å¸¸è§„å…‰æ …ä¸Šã€‚
#         plt.show()  # æ˜¾ç¤ºå›¾ç‰‡
#
#     show_img(BytesIO(graph.get_graph(xray=True).draw_mermaid_png(draw_method=MermaidDrawMethod.PYPPETEER)))
#
#
# show_graph()
```

å¯ä»¥ç»“åˆlangserveå¿«é€Ÿèµ·ä¸€ä¸ªapi

```python
add_routes(
    assistant_router,
    RunnableLambda(assistant_chat_stream).with_types(input_type=AssistantState),
    path="/ai/assistant",
)
```

### å¾…è¡¥å……
